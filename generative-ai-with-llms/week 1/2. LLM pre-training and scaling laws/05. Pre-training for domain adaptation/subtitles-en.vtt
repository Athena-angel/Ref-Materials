WEBVTT

1
00:00:00.650 --> 00:00:05.123
So far, I've emphasized that you'll
generally work with an existing LLM as you

2
00:00:05.123 --> 00:00:07.330
develop your application.

3
00:00:07.330 --> 00:00:12.850
This saves you a lot of time and can get
you to a working prototype much faster.

4
00:00:12.850 --> 00:00:17.502
However, there's one situation where you
may find it necessary to pretrain your own

5
00:00:17.502 --> 00:00:19.010
model from scratch.

6
00:00:19.010 --> 00:00:21.576
If your target domain uses vocabulary and

7
00:00:21.576 --> 00:00:25.987
language structures that are not
commonly used in day to day language.

8
00:00:25.987 --> 00:00:30.866
You may need to perform domain adaptation
to achieve good model performance.

9
00:00:30.866 --> 00:00:35.964
For example, imagine you're a developer
building an app to help lawyers and

10
00:00:35.964 --> 00:00:38.626
paralegals summarize legal briefs.

11
00:00:38.626 --> 00:00:43.263
Legal writing makes use of very
specific terms like mens rea in

12
00:00:43.263 --> 00:00:47.184
the first example and
res judicata in the second.

13
00:00:47.184 --> 00:00:51.769
These words are rarely used outside of
the legal world, which means that they

14
00:00:51.769 --> 00:00:56.474
are unlikely to have appeared widely
in the training text of existing LLMs.

15
00:00:56.474 --> 00:01:00.752
As a result, the models may have
difficulty understanding these terms or

16
00:01:00.752 --> 00:01:02.314
using them correctly.

17
00:01:02.314 --> 00:01:06.233
Another issue is that legal language
sometimes uses everyday words

18
00:01:06.233 --> 00:01:10.158
in a different context,
like consideration in the third example.

19
00:01:10.158 --> 00:01:14.957
Which has nothing to do with being nice,
but instead refers to the main element of

20
00:01:14.957 --> 00:01:18.146
a contract that makes
the agreement enforceable.

21
00:01:18.146 --> 00:01:19.554
For similar reasons,

22
00:01:19.554 --> 00:01:25.370
you may face challenges if you try to use
an existing LLM in a medical application.

23
00:01:25.370 --> 00:01:30.398
Medical language contains many uncommon
words to describe medical conditions and

24
00:01:30.398 --> 00:01:31.264
procedures.

25
00:01:31.264 --> 00:01:35.743
And these may not appear frequently
in training datasets consisting of

26
00:01:35.743 --> 00:01:37.686
web scrapes and book texts.

27
00:01:37.686 --> 00:01:41.648
Some domains also use language
in a highly idiosyncratic way.

28
00:01:41.648 --> 00:01:45.709
This last example of medical language
may just look like a string of random

29
00:01:45.709 --> 00:01:50.698
characters, but it's actually a shorthand
used by doctors to write prescriptions.

30
00:01:50.698 --> 00:01:54.170
This text has a very clear
meaning to a pharmacist,

31
00:01:54.170 --> 00:01:59.134
take one tablet by mouth four times a day,
after meals and at bedtime.

32
00:01:59.134 --> 00:02:01.444
Because models learn their vocabulary and

33
00:02:01.444 --> 00:02:05.174
understanding of language through
the original pretraining task.

34
00:02:05.174 --> 00:02:09.619
Pretraining your model from scratch
will result in better models for

35
00:02:09.619 --> 00:02:14.498
highly specialized domains like law,
medicine, finance or science.

36
00:02:14.498 --> 00:02:19.236
Now let's return to BloombergGPT,
first announced in 2023 in

37
00:02:19.236 --> 00:02:23.862
a paper by Shijie Wu, Steven Lu,
and colleagues at Bloomberg.

38
00:02:23.862 --> 00:02:28.527
BloombergGPT is an example of a large
language model that has been pretrained

39
00:02:28.527 --> 00:02:31.626
for a specific domain,
in this case, finance.

40
00:02:31.626 --> 00:02:35.933
The Bloomberg researchers chose
to combine both finance data and

41
00:02:35.933 --> 00:02:40.632
general purpose tax data to pretrain
a model that achieves Bestinclass

42
00:02:40.632 --> 00:02:42.996
results on financial benchmarks.

43
00:02:42.996 --> 00:02:47.574
While also maintaining competitive
performance on general purpose LLM

44
00:02:47.574 --> 00:02:48.654
benchmarks.

45
00:02:48.654 --> 00:02:54.553
As such, the researchers chose data
consisting of 51% financial data and

46
00:02:54.553 --> 00:02:56.392
49% public data.

47
00:02:56.392 --> 00:02:57.238
In their paper,

48
00:02:57.238 --> 00:03:01.372
the Bloomberg researchers describe
the model architecture in more detail.

49
00:03:01.372 --> 00:03:06.113
They also discuss how they started with
a chinchilla scaling laws for guidance and

50
00:03:06.113 --> 00:03:08.242
where they had to make tradeoffs.

51
00:03:08.242 --> 00:03:12.138
These two graphs compare a number of LLMs,
including BloombergGPT,

52
00:03:12.138 --> 00:03:15.526
to scaling laws that have been
discussed by researchers.

53
00:03:15.526 --> 00:03:20.101
On the left, the diagonal lines trace
the optimal model size in billions

54
00:03:20.101 --> 00:03:23.162
of parameters for
a range of compute budgets.

55
00:03:23.162 --> 00:03:27.676
On the right, the lines trace the compute
optimal training data set size

56
00:03:27.676 --> 00:03:29.770
measured in number of tokens.

57
00:03:29.770 --> 00:03:33.687
The dashed pink line on each graph
indicates the compute budget that

58
00:03:33.687 --> 00:03:37.560
the Bloomberg team had available for
training their new model.

59
00:03:37.560 --> 00:03:42.482
The pink shaded regions correspond to the
compute optimal scaling loss determined in

60
00:03:42.482 --> 00:03:44.230
the Chinchilla paper.

61
00:03:44.230 --> 00:03:48.507
In terms of model size, you can see
that BloombergGPT roughly follows

62
00:03:48.507 --> 00:03:53.580
the Chinchilla approach for the given
compute budget of 1.3 million GPU hours,

63
00:03:53.580 --> 00:03:56.786
or roughly 230,000,000 petaflops.

64
00:03:56.786 --> 00:03:59.936
The model is only a little bit
above the pink shaded region,

65
00:03:59.936 --> 00:04:03.542
suggesting the number of parameters
is fairly close to optimal.

66
00:04:03.542 --> 00:04:09.439
However, the actual number of tokens used
to pretrain BloombergGPT 569,000,000,000

67
00:04:09.439 --> 00:04:14.210
is below the recommended Chinchilla
value for the available compute budget.

68
00:04:14.210 --> 00:04:18.796
The smaller than optimal training data
set is due to the limited availability of

69
00:04:18.796 --> 00:04:20.239
financial domain data.

70
00:04:20.239 --> 00:04:24.419
Showing that real world constraints
may force you to make trade offs when

71
00:04:24.419 --> 00:04:26.950
pretraining your own models.

72
00:04:26.950 --> 00:04:30.052
Congratulations on making
it to the end of week one,

73
00:04:30.052 --> 00:04:35.272
you've covered a lot of ground, so let's
take a minute to recap what you've seen.

74
00:04:35.272 --> 00:04:38.891
Mike walked you through some of
the common use cases for LLMs,

75
00:04:38.891 --> 00:04:43.210
such as essay writing,
dialogue summarization and translation.

76
00:04:43.210 --> 00:04:47.662
He then gave a detailed presentation of
the transformer architecture that powers

77
00:04:47.662 --> 00:04:48.507
these models.

78
00:04:48.507 --> 00:04:52.830
And discussed some of the parameters you
can use at inference time to influence

79
00:04:52.830 --> 00:04:54.198
the model's output.

80
00:04:54.198 --> 00:04:58.818
He wrapped up by introducing you to
a generative AI project lifecycle that you

81
00:04:58.818 --> 00:05:02.708
can use to plan and
guide your application development work.

82
00:05:02.708 --> 00:05:06.784
Next, you saw how models are trained
on vast amounts of text data

83
00:05:06.784 --> 00:05:10.538
during an initial training
phase called pretraining.

84
00:05:10.538 --> 00:05:14.410
This is where models develop
their understanding of language.

85
00:05:14.410 --> 00:05:18.646
You explored some of the computational
challenges of training these models,

86
00:05:18.646 --> 00:05:20.001
which are significant.

87
00:05:20.001 --> 00:05:23.158
In practice because of
GPU memory limitations,

88
00:05:23.158 --> 00:05:28.546
you will almost always use some form of
quantization when training your models.

89
00:05:28.546 --> 00:05:33.568
You finish the week with a discussion of
scaling laws that have been discovered for

90
00:05:33.568 --> 00:05:37.586
LLMs and how they can be used to
design compute optimal models.

91
00:05:37.586 --> 00:05:39.530
If you'd like to read more of the details,

92
00:05:39.530 --> 00:05:41.970
be sure to check out this
week's reading exercises.