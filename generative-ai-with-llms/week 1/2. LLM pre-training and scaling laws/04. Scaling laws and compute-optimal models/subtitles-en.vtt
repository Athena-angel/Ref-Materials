WEBVTT

1
00:00:00.000 --> 00:00:01.485
In the last video,

2
00:00:01.485 --> 00:00:02.700
you explored some of

3
00:00:02.700 --> 00:00:04.950
the computational challenges of

4
00:00:04.950 --> 00:00:07.125
training large language models.

5
00:00:07.125 --> 00:00:09.960
Here you'll learn about
research that has

6
00:00:09.960 --> 00:00:13.260
explored the relationship
between model size, training,

7
00:00:13.260 --> 00:00:15.840
configuration and performance in

8
00:00:15.840 --> 00:00:19.335
an effort to determine just
how big models need to be.

9
00:00:19.335 --> 00:00:22.290
Remember, the goal during
pre-training is to

10
00:00:22.290 --> 00:00:24.405
maximize the model's performance

11
00:00:24.405 --> 00:00:26.070
of its learning objective,

12
00:00:26.070 --> 00:00:29.715
which is minimizing the loss
when predicting tokens.

13
00:00:29.715 --> 00:00:31.955
Two options you have to achieve

14
00:00:31.955 --> 00:00:34.880
better performance are
increasing the size of

15
00:00:34.880 --> 00:00:37.250
the dataset you
train your model on

16
00:00:37.250 --> 00:00:40.220
and increasing the number of
parameters in your model.

17
00:00:40.220 --> 00:00:42.650
In theory, you could
scale either of

18
00:00:42.650 --> 00:00:45.535
both of these quantities
to improve performance.

19
00:00:45.535 --> 00:00:48.230
However, another
issue to take into

20
00:00:48.230 --> 00:00:51.440
consideration is your
compute budget which

21
00:00:51.440 --> 00:00:54.440
includes factors like the
number of GPUs you have

22
00:00:54.440 --> 00:00:55.970
access to and the

23
00:00:55.970 --> 00:00:58.700
time you have available
for training models.

24
00:00:58.700 --> 00:01:01.790
To help you understand some
of the discussion ahead,

25
00:01:01.790 --> 00:01:03.710
let's first define a unit of

26
00:01:03.710 --> 00:01:06.950
compute that quantifies
the required resources.

27
00:01:06.950 --> 00:01:08.975
A petaFLOP per second day

28
00:01:08.975 --> 00:01:10.640
is a measurement
of the number of

29
00:01:10.640 --> 00:01:12.860
floating point
operations performed at

30
00:01:12.860 --> 00:01:15.530
a rate of one
petaFLOP per second,

31
00:01:15.530 --> 00:01:17.435
running for an entire day.

32
00:01:17.435 --> 00:01:20.360
Note, one petaFLOP
corresponds to

33
00:01:20.360 --> 00:01:23.530
one quadrillion floating
point operations per second.

34
00:01:23.530 --> 00:01:26.810
When specifically thinking
about training transformers,

35
00:01:26.810 --> 00:01:29.840
one petaFLOP per second
day is approximately

36
00:01:29.840 --> 00:01:33.065
equivalent to eight
NVIDIA V100 GPUs,

37
00:01:33.065 --> 00:01:36.515
operating at full efficiency
for one full day.

38
00:01:36.515 --> 00:01:38.600
If you have a more
powerful processor that

39
00:01:38.600 --> 00:01:40.880
can carry out more
operations at once,

40
00:01:40.880 --> 00:01:44.375
then a petaFLOP per second
day requires fewer chips.

41
00:01:44.375 --> 00:01:47.570
For example, two
NVIDIA A100 GPUs

42
00:01:47.570 --> 00:01:51.350
give equivalent compute
to the eight V100 chips.

43
00:01:51.350 --> 00:01:52.880
To give you an idea off

44
00:01:52.880 --> 00:01:54.935
the scale of these
compute budgets,

45
00:01:54.935 --> 00:01:56.915
this chart shows a comparison

46
00:01:56.915 --> 00:01:58.850
off the petaFLOP per second days

47
00:01:58.850 --> 00:02:01.415
required to pre-train
different variance

48
00:02:01.415 --> 00:02:03.185
of Bert and Roberta,

49
00:02:03.185 --> 00:02:05.645
which are both
encoder only models.

50
00:02:05.645 --> 00:02:09.230
T5 and encoder-decoder
model and GPT-3,

51
00:02:09.230 --> 00:02:11.015
which is a decoder only model.

52
00:02:11.015 --> 00:02:12.920
The difference
between the models in

53
00:02:12.920 --> 00:02:14.630
each family is the number

54
00:02:14.630 --> 00:02:16.445
of parameters that were trained,

55
00:02:16.445 --> 00:02:19.985
ranging from a few hundred
million for Bert base to

56
00:02:19.985 --> 00:02:23.995
175 billion for the
largest GPT-3 variant.

57
00:02:23.995 --> 00:02:26.670
Note that the y-axis
is logarithmic.

58
00:02:26.670 --> 00:02:29.930
Each increment vertically
is a power of 10.

59
00:02:29.930 --> 00:02:32.960
Here we see that T5 XL with

60
00:02:32.960 --> 00:02:35.030
three billion
parameters required

61
00:02:35.030 --> 00:02:37.655
close to 100 petaFLOP
per second days.

62
00:02:37.655 --> 00:02:42.155
While the larger GPT-3 175
billion parameter model

63
00:02:42.155 --> 00:02:47.240
required approximately 3,700
petaFLOP per second days.

64
00:02:47.240 --> 00:02:49.490
This chart makes it clear
that a huge amount of

65
00:02:49.490 --> 00:02:52.580
computers required to
train the largest models.

66
00:02:52.580 --> 00:02:54.560
You can see that bigger models

67
00:02:54.560 --> 00:02:56.240
take more compute resources to

68
00:02:56.240 --> 00:02:58.520
train and generally also require

69
00:02:58.520 --> 00:03:01.060
more data to achieve
good performance.

70
00:03:01.060 --> 00:03:03.155
It turns out that they are

71
00:03:03.155 --> 00:03:05.360
actually well-defined
relationships between

72
00:03:05.360 --> 00:03:07.295
these three scaling choices.

73
00:03:07.295 --> 00:03:09.530
Researchers have
explored the trade-offs

74
00:03:09.530 --> 00:03:11.420
between training dataset size,

75
00:03:11.420 --> 00:03:14.150
model size and compute budget.

76
00:03:14.150 --> 00:03:16.010
Here's a figure from a paper by

77
00:03:16.010 --> 00:03:17.930
researchers at OpenAI that

78
00:03:17.930 --> 00:03:19.820
explores the impact of compute

79
00:03:19.820 --> 00:03:22.310
budget on model performance.

80
00:03:22.310 --> 00:03:25.265
The y-axis is the test loss,

81
00:03:25.265 --> 00:03:27.170
which you can consider
as a proxy for

82
00:03:27.170 --> 00:03:30.485
model performance where
smaller values are better.

83
00:03:30.485 --> 00:03:32.990
The x-axis is the compute budget

84
00:03:32.990 --> 00:03:35.770
in units of petaFLOP
per second days.

85
00:03:35.770 --> 00:03:39.320
As you just saw, larger numbers
can be achieved by either

86
00:03:39.320 --> 00:03:43.595
using more compute power or
training for longer or both.

87
00:03:43.595 --> 00:03:45.680
Each thin blue line here shows

88
00:03:45.680 --> 00:03:48.460
the model loss over a
single training run.

89
00:03:48.460 --> 00:03:50.330
Looking at where
the loss starts to

90
00:03:50.330 --> 00:03:52.100
decline more slowly
for each run,

91
00:03:52.100 --> 00:03:54.050
reveals a clear
relationship between

92
00:03:54.050 --> 00:03:57.230
the compute budget and
the model's performance.

93
00:03:57.230 --> 00:04:00.665
This can be approximated by
a power-law relationship,

94
00:04:00.665 --> 00:04:02.890
shown by this pink line.

95
00:04:02.890 --> 00:04:05.855
A power law is a
mathematical relationship

96
00:04:05.855 --> 00:04:07.355
between two variables,

97
00:04:07.355 --> 00:04:09.080
where one is proportional to the

98
00:04:09.080 --> 00:04:10.865
other raised to some power.

99
00:04:10.865 --> 00:04:14.180
When plotted on a graph where
both axes are logarithmic,

100
00:04:14.180 --> 00:04:17.260
power-law relationships
appear as straight lines.

101
00:04:17.260 --> 00:04:20.450
The relationship here holds
as long as model size and

102
00:04:20.450 --> 00:04:23.965
training dataset size don't
inhibit the training process.

103
00:04:23.965 --> 00:04:25.610
Taken at face value,

104
00:04:25.610 --> 00:04:27.890
this would suggest that
you can just increase

105
00:04:27.890 --> 00:04:31.225
your compute budget to achieve
better model performance.

106
00:04:31.225 --> 00:04:32.645
In practice however,

107
00:04:32.645 --> 00:04:34.130
the compute resources you

108
00:04:34.130 --> 00:04:35.840
have available for training will

109
00:04:35.840 --> 00:04:38.360
generally be a hard
constraint set by

110
00:04:38.360 --> 00:04:41.315
factors such as the hardware
you have access to,

111
00:04:41.315 --> 00:04:43.505
the time available for training

112
00:04:43.505 --> 00:04:46.160
and the financial
budget of the project.

113
00:04:46.160 --> 00:04:48.830
If you hold your
compute budget fixed,

114
00:04:48.830 --> 00:04:50.150
the two levers you have to

115
00:04:50.150 --> 00:04:52.070
improve your model's
performance are

116
00:04:52.070 --> 00:04:53.930
the size of the training dataset

117
00:04:53.930 --> 00:04:56.540
and the number of
parameters in your model.

118
00:04:56.540 --> 00:04:58.700
The OpenAI researchers found

119
00:04:58.700 --> 00:05:00.650
that these two
quantities also show

120
00:05:00.650 --> 00:05:02.450
a power-law relationship with

121
00:05:02.450 --> 00:05:04.280
a test loss in the case where

122
00:05:04.280 --> 00:05:06.695
the other two variables
are held fixed.

123
00:05:06.695 --> 00:05:09.620
This is another figure
from the paper exploring

124
00:05:09.620 --> 00:05:13.490
the impact of training dataset
size on model performance.

125
00:05:13.490 --> 00:05:16.670
Here, the compute budget
and model size are held

126
00:05:16.670 --> 00:05:20.330
fixed and the size of the
training dataset is vary.

127
00:05:20.330 --> 00:05:21.710
The graph shows that as

128
00:05:21.710 --> 00:05:23.825
the volume of training
data increases,

129
00:05:23.825 --> 00:05:26.905
the performance of the
model continues to improve.

130
00:05:26.905 --> 00:05:28.490
In the second graph,

131
00:05:28.490 --> 00:05:30.050
the compute budget and

132
00:05:30.050 --> 00:05:32.360
training dataset size
are held constant.

133
00:05:32.360 --> 00:05:35.465
Models of varying numbers
of parameters are trained.

134
00:05:35.465 --> 00:05:37.430
As the model increases in size,

135
00:05:37.430 --> 00:05:41.350
the test loss decreases
indicating better performance.

136
00:05:41.350 --> 00:05:43.415
At this point you
might be asking,

137
00:05:43.415 --> 00:05:46.595
what's the ideal balance
between these three quantities?

138
00:05:46.595 --> 00:05:48.590
Well, it turns out a lot

139
00:05:48.590 --> 00:05:50.675
of people are interested
in this question.

140
00:05:50.675 --> 00:05:53.360
Both research and
industry communities

141
00:05:53.360 --> 00:05:54.530
have published a lot of

142
00:05:54.530 --> 00:05:58.585
empirical data for pre-training
compute optimal models.

143
00:05:58.585 --> 00:06:01.595
In a paper published in 2022,

144
00:06:01.595 --> 00:06:04.370
a group of researchers
led by Jordan Hoffmann,

145
00:06:04.370 --> 00:06:07.035
Sebastian Borgeaud
and Arthur Mensch

146
00:06:07.035 --> 00:06:08.660
carried out a detailed study of

147
00:06:08.660 --> 00:06:10.760
the performance of
language models of

148
00:06:10.760 --> 00:06:13.880
various sizes and quantities
of training data.

149
00:06:13.880 --> 00:06:16.820
The goal was to find the
optimal number of parameters

150
00:06:16.820 --> 00:06:20.465
and volume of training data
for a given compute budget.

151
00:06:20.465 --> 00:06:22.340
The author's name, the resulting

152
00:06:22.340 --> 00:06:24.830
compute optimal
model, Chinchilla.

153
00:06:24.830 --> 00:06:28.400
This paper is often referred
to as the Chinchilla paper.

154
00:06:28.400 --> 00:06:30.520
Let's take a look at
some of their findings.

155
00:06:30.520 --> 00:06:32.660
The Chinchilla paper hints that

156
00:06:32.660 --> 00:06:36.170
many of the 100 billion
parameter large language models

157
00:06:36.170 --> 00:06:40.175
like GPT-3 may actually
be over parameterized,

158
00:06:40.175 --> 00:06:42.440
meaning they have
more parameters than

159
00:06:42.440 --> 00:06:44.690
they need to achieve a
good understanding of

160
00:06:44.690 --> 00:06:47.510
language and under trained

161
00:06:47.510 --> 00:06:48.920
so that they would benefit from

162
00:06:48.920 --> 00:06:50.570
seeing more training data.

163
00:06:50.570 --> 00:06:52.580
The authors hypothesized that

164
00:06:52.580 --> 00:06:54.650
smaller models may
be able to achieve

165
00:06:54.650 --> 00:06:57.365
the same performance
as much larger ones

166
00:06:57.365 --> 00:07:00.115
if they are trained
on larger datasets.

167
00:07:00.115 --> 00:07:03.650
In this table, you can see
a selection of models along

168
00:07:03.650 --> 00:07:05.270
with their size and information

169
00:07:05.270 --> 00:07:07.370
about the dataset
they were trained on.

170
00:07:07.370 --> 00:07:11.210
One important takeaway from
the Chinchilla paper is that

171
00:07:11.210 --> 00:07:13.160
the optimal training
dataset size

172
00:07:13.160 --> 00:07:14.750
for a given model is about

173
00:07:14.750 --> 00:07:16.850
20 times larger than

174
00:07:16.850 --> 00:07:18.740
the number of parameters
in the model.

175
00:07:18.740 --> 00:07:22.355
Chinchilla was determined
to be compute optimal.

176
00:07:22.355 --> 00:07:24.815
For a 70 billion
parameter model,

177
00:07:24.815 --> 00:07:28.280
the ideal training
dataset contains 1.4

178
00:07:28.280 --> 00:07:32.240
trillion tokens or 20 times
the number of parameters.

179
00:07:32.240 --> 00:07:34.760
The last three models in
the table were trained on

180
00:07:34.760 --> 00:07:36.350
datasets that are smaller

181
00:07:36.350 --> 00:07:38.320
than the Chinchilla
optimal size.

182
00:07:38.320 --> 00:07:41.600
These models may actually
be under trained.

183
00:07:41.600 --> 00:07:44.150
In contrast, LLaMA
was trained on

184
00:07:44.150 --> 00:07:47.330
a dataset size of
1.4 trillion tokens,

185
00:07:47.330 --> 00:07:50.155
which is close to the
Chinchilla recommended number.

186
00:07:50.155 --> 00:07:53.390
Another important result
from the paper is that the

187
00:07:53.390 --> 00:07:56.570
compute optimal Chinchilla
model outperforms

188
00:07:56.570 --> 00:07:58.790
non compute optimal
models such as

189
00:07:58.790 --> 00:08:03.385
GPT-3 on a large range of
downstream evaluation tasks.

190
00:08:03.385 --> 00:08:05.550
With the results of
the Chinchilla paper

191
00:08:05.550 --> 00:08:06.950
in hand teams have

192
00:08:06.950 --> 00:08:08.270
recently started to develop

193
00:08:08.270 --> 00:08:10.505
smaller models that
achieved similar,

194
00:08:10.505 --> 00:08:12.470
if not better results than

195
00:08:12.470 --> 00:08:15.835
larger models that were
trained in a non-optimal way.

196
00:08:15.835 --> 00:08:18.170
Moving forward, you can probably

197
00:08:18.170 --> 00:08:20.510
expect to see a deviation
from the bigger is

198
00:08:20.510 --> 00:08:23.630
always better trends of
the last few years as

199
00:08:23.630 --> 00:08:25.430
more teams or developers like

200
00:08:25.430 --> 00:08:28.570
you start to optimize
their model design.

201
00:08:28.570 --> 00:08:30.890
The last model shown
on this slide,

202
00:08:30.890 --> 00:08:34.040
Bloomberg GPT, is a
really interesting model.

203
00:08:34.040 --> 00:08:36.950
It was trained in a compute
optimal way following

204
00:08:36.950 --> 00:08:39.050
the Chinchilla loss
and so achieves

205
00:08:39.050 --> 00:08:42.580
good performance with the size
of 50 billion parameters.

206
00:08:42.580 --> 00:08:45.830
It's also an interesting
example of a situation where

207
00:08:45.830 --> 00:08:47.780
pre-training a model
from scratch was

208
00:08:47.780 --> 00:08:50.345
necessary to achieve
good task performance.

209
00:08:50.345 --> 00:08:52.130
Let's move on to
the last video of

210
00:08:52.130 --> 00:08:54.570
this week to discuss why.