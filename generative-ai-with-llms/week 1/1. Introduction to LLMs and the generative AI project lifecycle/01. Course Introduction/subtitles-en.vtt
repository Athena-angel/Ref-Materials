WEBVTT

1
00:00:00.000 --> 00:00:01.560
Welcome to this course on

2
00:00:01.560 --> 00:00:04.845
generative AI with
large language models.

3
00:00:04.845 --> 00:00:06.960
Large language models or LLMs

4
00:00:06.960 --> 00:00:09.180
are a very exciting technology.

5
00:00:09.180 --> 00:00:11.475
But despite all
the buzz and hype,

6
00:00:11.475 --> 00:00:13.950
one of the thing that is
still underestimated by

7
00:00:13.950 --> 00:00:17.940
many people is their
power as a developer too.

8
00:00:17.940 --> 00:00:19.500
Specifically, there are

9
00:00:19.500 --> 00:00:21.210
many machine learning and AI

10
00:00:21.210 --> 00:00:23.040
applications that
used to take me

11
00:00:23.040 --> 00:00:25.515
many months to build
that you can now build

12
00:00:25.515 --> 00:00:28.395
in days or maybe even
small numbers of weeks.

13
00:00:28.395 --> 00:00:32.400
This course will take a
deep dive with you into how

14
00:00:32.400 --> 00:00:34.605
LLM technology actually works

15
00:00:34.605 --> 00:00:38.040
including going through many
of the technical details,

16
00:00:38.040 --> 00:00:42.175
like model training, instruction
tuning, fine-tuning,

17
00:00:42.175 --> 00:00:45.740
the generative AI project
life cycle framework to

18
00:00:45.740 --> 00:00:49.970
help you plan and execute
your projects and so on.

19
00:00:49.970 --> 00:00:52.190
Generative AI and LLMs

20
00:00:52.190 --> 00:00:55.445
specifically are a general
purpose technology.

21
00:00:55.445 --> 00:00:57.320
That means that similar to

22
00:00:57.320 --> 00:00:59.285
other general
purpose technologies

23
00:00:59.285 --> 00:01:01.940
like deep learning
and electricity,

24
00:01:01.940 --> 00:01:04.640
is useful not just for
a single application,

25
00:01:04.640 --> 00:01:05.690
but for a lot of

26
00:01:05.690 --> 00:01:07.550
different applications that span

27
00:01:07.550 --> 00:01:10.060
many corners of the economy.

28
00:01:10.060 --> 00:01:13.380
Similar to the rise
of deep learning that

29
00:01:13.380 --> 00:01:16.560
started maybe 15
years ago or so,

30
00:01:16.560 --> 00:01:19.850
there's a lot of important
where it lies ahead of us that

31
00:01:19.850 --> 00:01:23.090
needs to be done over many
years by many people,

32
00:01:23.090 --> 00:01:24.320
I hope including you,

33
00:01:24.320 --> 00:01:28.900
to identify use cases and
build specific applications.

34
00:01:28.900 --> 00:01:31.760
Because a lot of with
this technology is so

35
00:01:31.760 --> 00:01:34.855
new and so few people really
know how to use them,

36
00:01:34.855 --> 00:01:36.950
many companies are also right

37
00:01:36.950 --> 00:01:38.900
now scrambling to
try to find and

38
00:01:38.900 --> 00:01:40.805
hire people that actually know

39
00:01:40.805 --> 00:01:43.160
how to build
applications with LLMs.

40
00:01:43.160 --> 00:01:46.415
I hope that this course
will also help you,

41
00:01:46.415 --> 00:01:48.290
if you wish, better position

42
00:01:48.290 --> 00:01:50.525
yourself to get
one of those jobs.

43
00:01:50.525 --> 00:01:53.000
I'm thrilled to bring you
this course along with

44
00:01:53.000 --> 00:01:56.225
a group of fantastic
instructors from the AWS team,

45
00:01:56.225 --> 00:02:01.700
[inaudible] who are
here with me today,

46
00:02:01.700 --> 00:02:04.565
as well as a fourth
instructor Chris Freby,

47
00:02:04.565 --> 00:02:06.515
who will be presenting
sort of adds.

48
00:02:06.515 --> 00:02:11.530
Andrea and Mike above generative
AI developer advocates.

49
00:02:11.530 --> 00:02:13.715
Shelby and Chris above

50
00:02:13.715 --> 00:02:16.130
generative AI
solutions architects.

51
00:02:16.130 --> 00:02:17.720
All of them have a lot of

52
00:02:17.720 --> 00:02:21.050
experience helping many
different companies build many,

53
00:02:21.050 --> 00:02:23.480
many creative
applications using LLMs.

54
00:02:23.480 --> 00:02:26.300
I look forward to
all of them sharing

55
00:02:26.300 --> 00:02:29.740
this rich hands-on
experience in this course.

56
00:02:29.740 --> 00:02:31.400
We've develop the content for

57
00:02:31.400 --> 00:02:33.230
this course with inputs from

58
00:02:33.230 --> 00:02:37.640
many industry experts and
applied scientists at Amazon,

59
00:02:37.640 --> 00:02:39.620
AWS, Hugging Face and

60
00:02:39.620 --> 00:02:41.740
many top universities
around the world.

61
00:02:41.740 --> 00:02:43.160
Andrea, perhaps you can say a

62
00:02:43.160 --> 00:02:44.600
bit more about this course.

63
00:02:44.600 --> 00:02:45.905
Sure. Thanks Andrew.

64
00:02:45.905 --> 00:02:48.560
It's a pleasure to work with
you again on this course and

65
00:02:48.560 --> 00:02:51.290
the exciting area
of generative AI.

66
00:02:51.290 --> 00:02:52.460
With this course on

67
00:02:52.460 --> 00:02:54.935
generative AI with
large language models,

68
00:02:54.935 --> 00:02:56.750
we've created a series of

69
00:02:56.750 --> 00:02:59.150
lessons meant for
AI enthusiasts,

70
00:02:59.150 --> 00:03:01.260
engineers, or data scientists.

71
00:03:01.260 --> 00:03:03.995
Looking to learn the
technical foundations

72
00:03:03.995 --> 00:03:05.750
of how LLMs work,

73
00:03:05.750 --> 00:03:08.555
as well as the best
practices behind training,

74
00:03:08.555 --> 00:03:10.550
tuning, and deploying them.

75
00:03:10.550 --> 00:03:12.050
In terms of prerequisites,

76
00:03:12.050 --> 00:03:14.120
we assume you are
already familiar with

77
00:03:14.120 --> 00:03:16.340
Python programming and at least

78
00:03:16.340 --> 00:03:19.445
basic data science and
machine learning concepts.

79
00:03:19.445 --> 00:03:21.230
If you have some experience with

80
00:03:21.230 --> 00:03:24.560
either Python or TensorFlow,
that should be enough.

81
00:03:24.560 --> 00:03:28.520
In this course, you will explore
in detail the steps that

82
00:03:28.520 --> 00:03:32.305
make up a typical generative
AI project lifecycle,

83
00:03:32.305 --> 00:03:34.040
from scoping the problem and

84
00:03:34.040 --> 00:03:36.290
selecting a language model to

85
00:03:36.290 --> 00:03:37.490
optimizing a model for

86
00:03:37.490 --> 00:03:40.475
deployment and integrating
into your applications.

87
00:03:40.475 --> 00:03:42.710
This course covers
all of the topics,

88
00:03:42.710 --> 00:03:44.315
not just at a shallow level,

89
00:03:44.315 --> 00:03:46.880
but we'll take the time
to make sure you come

90
00:03:46.880 --> 00:03:49.850
away with a deep technical
understanding of

91
00:03:49.850 --> 00:03:53.240
all of these technologies
and be well-positioned to

92
00:03:53.240 --> 00:03:55.190
really know what
you're doing when you

93
00:03:55.190 --> 00:03:57.260
build your own
generative AI projects.

94
00:03:57.260 --> 00:03:58.730
Mike, why don't you tell us

95
00:03:58.730 --> 00:04:00.185
a little bit more details about

96
00:04:00.185 --> 00:04:01.970
what the learners will
see in each week?

97
00:04:01.970 --> 00:04:04.275
Absolutely, Andrea. Thank you.

98
00:04:04.275 --> 00:04:06.500
In Week 1, you will examine

99
00:04:06.500 --> 00:04:08.450
the transformer
architecture that

100
00:04:08.450 --> 00:04:10.595
powers large language models,

101
00:04:10.595 --> 00:04:13.220
explore how these
models are trained,

102
00:04:13.220 --> 00:04:15.710
and understand the
compute resources

103
00:04:15.710 --> 00:04:18.815
required to develop
these powerful LLMs.

104
00:04:18.815 --> 00:04:20.600
You'll also learn about

105
00:04:20.600 --> 00:04:23.465
a technique called
in-context learning.

106
00:04:23.465 --> 00:04:26.045
How to guide the model to output

107
00:04:26.045 --> 00:04:29.045
at inference time with
prompt engineering,

108
00:04:29.045 --> 00:04:30.680
and how to tune

109
00:04:30.680 --> 00:04:33.275
the most important
generation parameters

110
00:04:33.275 --> 00:04:36.350
of LLMs for tuning
your model output.

111
00:04:36.350 --> 00:04:37.610
In Week 2,

112
00:04:37.610 --> 00:04:41.450
you'll explore options for
adapting pre-trained models to

113
00:04:41.450 --> 00:04:44.000
specific tasks and datasets via

114
00:04:44.000 --> 00:04:47.270
a process called
instruction fine tuning.

115
00:04:47.270 --> 00:04:48.680
Then in Week 3,

116
00:04:48.680 --> 00:04:50.120
you'll see how to align

117
00:04:50.120 --> 00:04:52.445
the output of
language models with

118
00:04:52.445 --> 00:04:54.890
human values in
order to increase

119
00:04:54.890 --> 00:04:58.850
helpfulness and decrease
potential harm and toxicity.

120
00:04:58.850 --> 00:05:01.025
Though we don't
stop at the theory.

121
00:05:01.025 --> 00:05:03.680
Each week includes
a hands-on lab

122
00:05:03.680 --> 00:05:06.080
where you'll be able to try
out these techniques for

123
00:05:06.080 --> 00:05:09.605
yourself in an AWS
environment that includes

124
00:05:09.605 --> 00:05:12.290
all the resources you
need for working with

125
00:05:12.290 --> 00:05:15.395
large models at no cost to you.

126
00:05:15.395 --> 00:05:16.850
Shelby, can you tell us a little

127
00:05:16.850 --> 00:05:18.350
bit more about the
hands-on labs?

128
00:05:18.350 --> 00:05:21.665
Sure thing, Mike. In
the first hands-on lab,

129
00:05:21.665 --> 00:05:22.970
you'll construct a compare

130
00:05:22.970 --> 00:05:25.745
different prompts and inputs
for a given generative task,

131
00:05:25.745 --> 00:05:27.725
in this case, dialogue
summarization.

132
00:05:27.725 --> 00:05:29.960
You'll also explore different
inference parameters

133
00:05:29.960 --> 00:05:31.670
and sampling strategies to gain

134
00:05:31.670 --> 00:05:33.500
intuition on how to further

135
00:05:33.500 --> 00:05:36.055
improve the generative
model of responses.

136
00:05:36.055 --> 00:05:37.910
In the second hands-on lab,

137
00:05:37.910 --> 00:05:39.440
you'll find tune it existing

138
00:05:39.440 --> 00:05:41.420
large language model
from Hugging Face,

139
00:05:41.420 --> 00:05:43.700
a popular open-source model hub.

140
00:05:43.700 --> 00:05:46.310
You'll play with both
full fine-tuning and

141
00:05:46.310 --> 00:05:49.220
parameter efficient fine
tuning or PEFT for short.

142
00:05:49.220 --> 00:05:51.170
You'll see how puffed lets you

143
00:05:51.170 --> 00:05:53.530
make your workflow
much more efficient.

144
00:05:53.530 --> 00:05:56.420
In the third lab, you get
hands-on with reinforcement

145
00:05:56.420 --> 00:05:59.105
learning from human
feedback or RLE Jeff,

146
00:05:59.105 --> 00:06:01.910
you'll build a reward
model classifier to label

147
00:06:01.910 --> 00:06:04.865
model responses as either
toxic or non-toxic.

148
00:06:04.865 --> 00:06:06.290
Don't worry if you
don't understand

149
00:06:06.290 --> 00:06:08.250
all these terms and
concepts just yet.

150
00:06:08.250 --> 00:06:10.100
You'll dive into each
of these topics in

151
00:06:10.100 --> 00:06:12.445
much more detail
throughout this course.

152
00:06:12.445 --> 00:06:15.345
I'm thrilled to
have Andrea, Mike,

153
00:06:15.345 --> 00:06:18.740
Shelby as well as Tris
presenting this course to

154
00:06:18.740 --> 00:06:22.670
you that takes a deep
technical dive into LLMs.

155
00:06:22.670 --> 00:06:25.850
You come away from this
course having practice with

156
00:06:25.850 --> 00:06:27.980
many different
concrete code examples

157
00:06:27.980 --> 00:06:29.750
for how to build or use LLMs.

158
00:06:29.750 --> 00:06:32.390
I'm sure that many of
the code snippets will

159
00:06:32.390 --> 00:06:35.405
end up being directly
useful in your own work.

160
00:06:35.405 --> 00:06:38.120
I hope you enjoy the
course and use what

161
00:06:38.120 --> 00:06:41.015
you learn to build some
really exciting applications.

162
00:06:41.015 --> 00:06:44.240
So that, let's go on to the
next video where we start

163
00:06:44.240 --> 00:06:46.190
diving into how LLMs

164
00:06:46.190 --> 00:06:49.290
are being used to
build applications.