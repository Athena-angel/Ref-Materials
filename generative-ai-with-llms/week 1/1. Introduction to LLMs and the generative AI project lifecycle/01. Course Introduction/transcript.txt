Welcome to this course on generative AI with
large language models. Large language models or LLMs are a very exciting technology. But despite all
the buzz and hype, one of the thing that is
still underestimated by many people is their
power as a developer too. Specifically, there are many machine learning and AI applications that
used to take me many months to build
that you can now build in days or maybe even
small numbers of weeks. This course will take a
deep dive with you into how LLM technology actually works including going through many
of the technical details, like model training, instruction
tuning, fine-tuning, the generative AI project
life cycle framework to help you plan and execute
your projects and so on. Generative AI and LLMs specifically are a general
purpose technology. That means that similar to other general
purpose technologies like deep learning
and electricity, is useful not just for
a single application, but for a lot of different applications that span many corners of the economy. Similar to the rise
of deep learning that started maybe 15
years ago or so, there's a lot of important
where it lies ahead of us that needs to be done over many
years by many people, I hope including you, to identify use cases and
build specific applications. Because a lot of with
this technology is so new and so few people really
know how to use them, many companies are also right now scrambling to
try to find and hire people that actually know how to build
applications with LLMs. I hope that this course
will also help you, if you wish, better position yourself to get
one of those jobs. I'm thrilled to bring you
this course along with a group of fantastic
instructors from the AWS team, [inaudible] who are
here with me today, as well as a fourth
instructor Chris Freby, who will be presenting
sort of adds. Andrea and Mike above generative
AI developer advocates. Shelby and Chris above generative AI
solutions architects. All of them have a lot of experience helping many
different companies build many, many creative
applications using LLMs. I look forward to
all of them sharing this rich hands-on
experience in this course. We've develop the content for this course with inputs from many industry experts and
applied scientists at Amazon, AWS, Hugging Face and many top universities
around the world. Andrea, perhaps you can say a bit more about this course. Sure. Thanks Andrew. It's a pleasure to work with
you again on this course and the exciting area
of generative AI. With this course on generative AI with
large language models, we've created a series of lessons meant for
AI enthusiasts, engineers, or data scientists. Looking to learn the
technical foundations of how LLMs work, as well as the best
practices behind training, tuning, and deploying them. In terms of prerequisites, we assume you are
already familiar with Python programming and at least basic data science and
machine learning concepts. If you have some experience with either Python or TensorFlow,
that should be enough. In this course, you will explore
in detail the steps that make up a typical generative
AI project lifecycle, from scoping the problem and selecting a language model to optimizing a model for deployment and integrating
into your applications. This course covers
all of the topics, not just at a shallow level, but we'll take the time
to make sure you come away with a deep technical
understanding of all of these technologies
and be well-positioned to really know what
you're doing when you build your own
generative AI projects. Mike, why don't you tell us a little bit more details about what the learners will
see in each week? Absolutely, Andrea. Thank you. In Week 1, you will examine the transformer
architecture that powers large language models, explore how these
models are trained, and understand the
compute resources required to develop
these powerful LLMs. You'll also learn about a technique called
in-context learning. How to guide the model to output at inference time with
prompt engineering, and how to tune the most important
generation parameters of LLMs for tuning
your model output. In Week 2, you'll explore options for
adapting pre-trained models to specific tasks and datasets via a process called
instruction fine tuning. Then in Week 3, you'll see how to align the output of
language models with human values in
order to increase helpfulness and decrease
potential harm and toxicity. Though we don't
stop at the theory. Each week includes
a hands-on lab where you'll be able to try
out these techniques for yourself in an AWS
environment that includes all the resources you
need for working with large models at no cost to you. Shelby, can you tell us a little bit more about the
hands-on labs? Sure thing, Mike. In
the first hands-on lab, you'll construct a compare different prompts and inputs
for a given generative task, in this case, dialogue
summarization. You'll also explore different
inference parameters and sampling strategies to gain intuition on how to further improve the generative
model of responses. In the second hands-on lab, you'll find tune it existing large language model
from Hugging Face, a popular open-source model hub. You'll play with both
full fine-tuning and parameter efficient fine
tuning or PEFT for short. You'll see how puffed lets you make your workflow
much more efficient. In the third lab, you get
hands-on with reinforcement learning from human
feedback or RLE Jeff, you'll build a reward
model classifier to label model responses as either
toxic or non-toxic. Don't worry if you
don't understand all these terms and
concepts just yet. You'll dive into each
of these topics in much more detail
throughout this course. I'm thrilled to
have Andrea, Mike, Shelby as well as Tris
presenting this course to you that takes a deep
technical dive into LLMs. You come away from this
course having practice with many different
concrete code examples for how to build or use LLMs. I'm sure that many of
the code snippets will end up being directly
useful in your own work. I hope you enjoy the
course and use what you learn to build some
really exciting applications. So that, let's go on to the
next video where we start diving into how LLMs are being used to
build applications.