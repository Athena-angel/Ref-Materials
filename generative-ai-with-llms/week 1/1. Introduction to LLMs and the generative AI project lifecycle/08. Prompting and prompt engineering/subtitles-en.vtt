WEBVTT

1
00:00:00.000 --> 00:00:03.825
Okay, Just to remind you of
some of the terminology.

2
00:00:03.825 --> 00:00:05.760
The text that you feed into

3
00:00:05.760 --> 00:00:07.980
the model is called the prompt,

4
00:00:07.980 --> 00:00:11.565
the act of generating text
is known as inference,

5
00:00:11.565 --> 00:00:15.225
and the output text is
known as the completion.

6
00:00:15.225 --> 00:00:18.210
The full amount of text or
the memory that is available

7
00:00:18.210 --> 00:00:21.255
to use for the prompt is
called the context window.

8
00:00:21.255 --> 00:00:22.935
Although the example here

9
00:00:22.935 --> 00:00:24.750
shows the model performing well,

10
00:00:24.750 --> 00:00:28.140
you'll frequently encounter
situations where the model

11
00:00:28.140 --> 00:00:29.625
doesn't produce the
outcome that you

12
00:00:29.625 --> 00:00:31.800
want on the first try.

13
00:00:31.800 --> 00:00:33.890
You may have to
revise the language

14
00:00:33.890 --> 00:00:35.720
in your prompt or
the way that it's

15
00:00:35.720 --> 00:00:37.550
written several times to get

16
00:00:37.550 --> 00:00:40.160
the model to behave in
the way that you want.

17
00:00:40.160 --> 00:00:42.620
This work to develop and improve

18
00:00:42.620 --> 00:00:45.365
the prompt is known as
prompt engineering.

19
00:00:45.365 --> 00:00:46.890
This is a big topic.

20
00:00:46.890 --> 00:00:48.920
But one powerful strategy to get

21
00:00:48.920 --> 00:00:51.350
the model to produce
better outcomes is to

22
00:00:51.350 --> 00:00:53.840
include examples of
the task that you want

23
00:00:53.840 --> 00:00:56.875
the model to carry out
inside the prompt.

24
00:00:56.875 --> 00:01:00.065
Providing examples inside
the context window

25
00:01:00.065 --> 00:01:02.605
is called in-context learning.

26
00:01:02.605 --> 00:01:05.475
Let's take a look at
what this term means.

27
00:01:05.475 --> 00:01:07.865
With in-context
learning, you can

28
00:01:07.865 --> 00:01:10.595
help LLMs learn more
about the task being

29
00:01:10.595 --> 00:01:13.025
asked by including examples

30
00:01:13.025 --> 00:01:16.055
or additional data
in the prompt.

31
00:01:16.055 --> 00:01:18.050
Here is a concrete example.

32
00:01:18.050 --> 00:01:19.595
Within the prompt shown here,

33
00:01:19.595 --> 00:01:23.510
you ask the model to classify
the sentiment of a review.

34
00:01:23.510 --> 00:01:25.565
So whether the
review of this movie

35
00:01:25.565 --> 00:01:27.920
is positive or negative,

36
00:01:27.920 --> 00:01:30.515
the prompt consists
of the instruction,

37
00:01:30.515 --> 00:01:34.970
"Classify this review,"
followed by some context,

38
00:01:34.970 --> 00:01:38.045
which in this case is
the review text itself,

39
00:01:38.045 --> 00:01:41.915
and an instruction to produce
the sentiment at the end.

40
00:01:41.915 --> 00:01:45.455
This method, including your
input data within the prompt,

41
00:01:45.455 --> 00:01:47.885
is called zero-shot inference.

42
00:01:47.885 --> 00:01:51.335
The largest of the LLMs are
surprisingly good at this,

43
00:01:51.335 --> 00:01:52.820
grasping the task to be

44
00:01:52.820 --> 00:01:55.260
completed and returning
a good answer.

45
00:01:55.260 --> 00:01:57.470
In this example,
the model correctly

46
00:01:57.470 --> 00:01:59.920
identifies the
sentiment as positive.

47
00:01:59.920 --> 00:02:01.520
Smaller models, on
the other hand,

48
00:02:01.520 --> 00:02:02.680
can struggle with this.

49
00:02:02.680 --> 00:02:06.920
Here's an example of a
completion generated by GPT-2,

50
00:02:06.920 --> 00:02:09.080
an earlier smaller version

51
00:02:09.080 --> 00:02:11.045
of the model that
powers ChatGPT.

52
00:02:11.045 --> 00:02:14.395
As you can see, the model
doesn't follow the instruction.

53
00:02:14.395 --> 00:02:16.100
While it does generate text

54
00:02:16.100 --> 00:02:17.750
with some relation
to the prompt,

55
00:02:17.750 --> 00:02:20.420
the model can't figure
out the details

56
00:02:20.420 --> 00:02:23.465
of the task and does not
identify the sentiment.

57
00:02:23.465 --> 00:02:25.595
This is where
providing an example

58
00:02:25.595 --> 00:02:28.300
within the prompt can
improve performance.

59
00:02:28.300 --> 00:02:30.215
Here you can see
that the prompt text

60
00:02:30.215 --> 00:02:32.465
is longer and now starts with

61
00:02:32.465 --> 00:02:34.400
a completed example that

62
00:02:34.400 --> 00:02:37.195
demonstrates the tasks to be
carried out to the model.

63
00:02:37.195 --> 00:02:38.690
After specifying that the model

64
00:02:38.690 --> 00:02:40.500
should classify the review,

65
00:02:40.500 --> 00:02:43.955
the prompt text includes
a sample review.

66
00:02:43.955 --> 00:02:45.500
I loved this movie,

67
00:02:45.500 --> 00:02:48.755
followed by a completed
sentiment analysis.

68
00:02:48.755 --> 00:02:51.205
In this case, the
review is positive.

69
00:02:51.205 --> 00:02:52.940
Next, the prompt states

70
00:02:52.940 --> 00:02:54.920
the instruction
again and includes

71
00:02:54.920 --> 00:02:56.945
the actual input review

72
00:02:56.945 --> 00:02:59.030
that we want the
model to analyze.

73
00:02:59.030 --> 00:03:02.720
You pass this new longer
prompt to the smaller model,

74
00:03:02.720 --> 00:03:04.370
which now has a better chance

75
00:03:04.370 --> 00:03:05.930
of understanding the task you're

76
00:03:05.930 --> 00:03:09.740
specifying and the format of
the response that you want.

77
00:03:09.740 --> 00:03:12.260
The inclusion of
a single example

78
00:03:12.260 --> 00:03:15.055
is known as one-shot inference,

79
00:03:15.055 --> 00:03:18.950
in contrast to the zero-shot
prompt you supplied earlier.

80
00:03:18.950 --> 00:03:21.710
Sometimes a single
example won't be

81
00:03:21.710 --> 00:03:24.860
enough for the model to learn
what you want it to do.

82
00:03:24.860 --> 00:03:27.020
So you can extend
the idea of giving

83
00:03:27.020 --> 00:03:30.710
a single example to
include multiple examples.

84
00:03:30.710 --> 00:03:33.715
This is known as
few-shot inference.

85
00:03:33.715 --> 00:03:35.090
Here, you're working with

86
00:03:35.090 --> 00:03:37.430
an even smaller
model that failed

87
00:03:37.430 --> 00:03:39.395
to carry out good
sentiment analysis

88
00:03:39.395 --> 00:03:41.255
with one-shot inference.

89
00:03:41.255 --> 00:03:43.190
Instead, you're going to try

90
00:03:43.190 --> 00:03:46.795
few-shot inference by
including a second example.

91
00:03:46.795 --> 00:03:49.050
This time, a negative review,

92
00:03:49.050 --> 00:03:51.200
including a mix of examples with

93
00:03:51.200 --> 00:03:53.300
different output
classes can help

94
00:03:53.300 --> 00:03:56.020
the model to understand
what it needs to do.

95
00:03:56.020 --> 00:03:58.575
You pass the new
prompts to the model.

96
00:03:58.575 --> 00:04:00.470
And this time it understands

97
00:04:00.470 --> 00:04:03.020
the instruction and
generates a completion that

98
00:04:03.020 --> 00:04:05.120
correctly identifies
the sentiment of

99
00:04:05.120 --> 00:04:07.495
the review as negative.

100
00:04:07.495 --> 00:04:10.430
So to recap, you can engineer

101
00:04:10.430 --> 00:04:14.695
your prompts to encourage the
model to learn by examples.

102
00:04:14.695 --> 00:04:16.460
While the largest
models are good at

103
00:04:16.460 --> 00:04:19.235
zero-shot inference
with no examples,

104
00:04:19.235 --> 00:04:22.610
smaller models can
benefit from one-shot or

105
00:04:22.610 --> 00:04:24.575
few-shot inference that include

106
00:04:24.575 --> 00:04:26.905
examples of the
desired behavior.

107
00:04:26.905 --> 00:04:30.950
But remember the context window
because you have a limit

108
00:04:30.950 --> 00:04:33.215
on the amount of
in-context learning

109
00:04:33.215 --> 00:04:35.140
that you can pass
into the model.

110
00:04:35.140 --> 00:04:36.995
Generally, if you
find that your model

111
00:04:36.995 --> 00:04:39.140
isn't performing well when, say,

112
00:04:39.140 --> 00:04:41.330
including five or six examples,

113
00:04:41.330 --> 00:04:44.255
you should try fine-tuning
your model instead.

114
00:04:44.255 --> 00:04:46.760
Fine-tuning performs
additional training

115
00:04:46.760 --> 00:04:47.855
on the model using

116
00:04:47.855 --> 00:04:49.610
new data to make it more

117
00:04:49.610 --> 00:04:51.890
capable of the task you
want it to perform.

118
00:04:51.890 --> 00:04:54.020
You'll explore fine-tuning in

119
00:04:54.020 --> 00:04:56.435
detail in week 2 of this course.

120
00:04:56.435 --> 00:04:59.390
As larger and larger
models have been trained,

121
00:04:59.390 --> 00:05:01.520
it's become clear
that the ability

122
00:05:01.520 --> 00:05:03.935
of models to perform
multiple tasks

123
00:05:03.935 --> 00:05:06.485
and how well they
perform those tasks

124
00:05:06.485 --> 00:05:09.335
depends strongly on the
scale of the model.

125
00:05:09.335 --> 00:05:11.480
As you heard earlier
in the lesson,

126
00:05:11.480 --> 00:05:13.970
models with more
parameters are able to

127
00:05:13.970 --> 00:05:16.975
capture more understanding
of language.

128
00:05:16.975 --> 00:05:19.325
The largest models
are surprisingly good

129
00:05:19.325 --> 00:05:21.935
at zero-shot inference
and are able to

130
00:05:21.935 --> 00:05:24.125
infer and successfully complete

131
00:05:24.125 --> 00:05:25.640
many tasks that they were

132
00:05:25.640 --> 00:05:27.740
not specifically
trained to perform.

133
00:05:27.740 --> 00:05:30.080
In contrast, smaller models are

134
00:05:30.080 --> 00:05:33.155
generally only good at a
small number of tasks.

135
00:05:33.155 --> 00:05:34.520
Typically, those that are

136
00:05:34.520 --> 00:05:36.860
similar to the task that
they were trained on.

137
00:05:36.860 --> 00:05:38.255
You may have to try out

138
00:05:38.255 --> 00:05:41.645
a few models to find the
right one for your use case.

139
00:05:41.645 --> 00:05:44.135
Once you've found the model
that is working for you,

140
00:05:44.135 --> 00:05:46.955
there are a few settings that
you can experiment with to

141
00:05:46.955 --> 00:05:49.040
influence the
structure and style

142
00:05:49.040 --> 00:05:51.350
of the completions that
the model generates.

143
00:05:51.350 --> 00:05:52.910
Let's take a look
at some of these

144
00:05:52.910 --> 00:05:56.190
configuration settings
in the next video.