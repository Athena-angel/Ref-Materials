WEBVTT

1
00:00:00.551 --> 00:00:04.874
Okay, let's get started, in this lesson,
we're going to set the scene.

2
00:00:04.874 --> 00:00:09.167
We'll talk about large language models,
their use cases,

3
00:00:09.167 --> 00:00:14.807
how the models work, prompt engineering,
how to make creative text outputs,

4
00:00:14.807 --> 00:00:19.370
and outline a project lifecycle for
generative AI projects.

5
00:00:19.370 --> 00:00:22.684
Given your interest in this course,
it's probably safe

6
00:00:22.684 --> 00:00:27.372
to say that you've had a chance to try out
a generative AI tool or would like to.

7
00:00:27.372 --> 00:00:31.807
Whether it be a chat bot,
generating images from text, or

8
00:00:31.807 --> 00:00:37.780
using a plugin to help you develop code,
what you see in these tools is a machine

9
00:00:37.780 --> 00:00:44.134
that is capable of creating content that
mimics or approximates human ability.

10
00:00:44.134 --> 00:00:47.871
Generative AI is a subset of
traditional machine learning.

11
00:00:47.871 --> 00:00:52.558
And the machine learning models
that underpin generative AI have

12
00:00:52.558 --> 00:00:57.502
learned these abilities by finding
statistical patterns in massive

13
00:00:57.502 --> 00:01:02.036
datasets of content that was
originally generated by humans.

14
00:01:02.036 --> 00:01:07.221
Large language models have been trained
on trillions of words over many weeks and

15
00:01:07.221 --> 00:01:10.512
months, and
with large amounts of compute power.

16
00:01:10.512 --> 00:01:15.444
These foundation models, as we call them,
with billions of parameters,

17
00:01:15.444 --> 00:01:20.147
exhibit emergent properties beyond
language alone, and researchers

18
00:01:20.147 --> 00:01:26.130
are unlocking their ability to break down
complex tasks, reason, and problem solve.

19
00:01:26.130 --> 00:01:31.102
Here are a collection of foundation
models, sometimes called base models,

20
00:01:31.102 --> 00:01:34.790
and their relative size in
terms of their parameters.

21
00:01:34.790 --> 00:01:39.265
You'll cover these parameters in
a little more detail later on, but

22
00:01:39.265 --> 00:01:42.506
for now,
think of them as the model's memory.

23
00:01:42.506 --> 00:01:46.447
And the more parameters a model has,
the more memory, and

24
00:01:46.447 --> 00:01:51.330
as it turns out, the more sophisticated
the tasks it can perform.

25
00:01:51.330 --> 00:01:56.313
Throughout this course, we'll represent
LLMs with these purple circles, and

26
00:01:56.313 --> 00:02:01.001
in the labs, you'll make use of
a specific open source model, flan-T5,

27
00:02:01.001 --> 00:02:02.851
to carry out language tasks.

28
00:02:02.851 --> 00:02:07.092
By either using these models as
they are or by applying fine tuning

29
00:02:07.092 --> 00:02:11.872
techniques to adapt them to your
specific use case, you can rapidly build

30
00:02:11.872 --> 00:02:16.802
customized solutions without the need
to train a new model from scratch.

31
00:02:16.802 --> 00:02:20.736
Now, while generative AI
models are being created for

32
00:02:20.736 --> 00:02:25.356
multiple modalities,
including images, video, audio, and

33
00:02:25.356 --> 00:02:30.146
speech, in this course you'll
focus on large language models and

34
00:02:30.146 --> 00:02:33.594
their uses in natural language generation.

35
00:02:33.594 --> 00:02:36.592
You will see how they are built and
trained,

36
00:02:36.592 --> 00:02:40.492
how you can interact with them
via text known as prompts.

37
00:02:40.492 --> 00:02:44.393
And how to fine tune models for
your use case and data, and

38
00:02:44.393 --> 00:02:50.222
how you can deploy them with applications
to solve your business and social tasks.

39
00:02:50.222 --> 00:02:54.698
The way you interact with language models
is quite different than other machine

40
00:02:54.698 --> 00:02:57.042
learning and programming paradigms.

41
00:02:57.042 --> 00:03:00.920
In those cases,
you write computer code with formalized

42
00:03:00.920 --> 00:03:04.146
syntax to interact with libraries and
APIs.

43
00:03:04.146 --> 00:03:09.228
In contrast, large language models
are able to take natural language or

44
00:03:09.228 --> 00:03:14.192
human written instructions and
perform tasks much as a human would.

45
00:03:14.192 --> 00:03:18.886
The text that you pass to
an LLM is known as a prompt.

46
00:03:18.886 --> 00:03:24.075
The space or memory that is available to
the prompt is called the context window,

47
00:03:24.075 --> 00:03:28.197
and this is typically large enough for
a few thousand words, but

48
00:03:28.197 --> 00:03:30.308
differs from model to model.

49
00:03:30.308 --> 00:03:31.396
In this example,

50
00:03:31.396 --> 00:03:36.232
you ask the model to determine where
Ganymede is located in the solar system.

51
00:03:36.232 --> 00:03:41.070
The prompt is passed to the model, the
model then predicts the next words, and

52
00:03:41.070 --> 00:03:46.024
because your prompt contained a question,
this model generates an answer.

53
00:03:46.024 --> 00:03:49.236
The output of the model
is called a completion,

54
00:03:49.236 --> 00:03:54.098
and the act of using the model to
generate text is known as inference.

55
00:03:54.098 --> 00:03:59.201
The completion is comprised of the text
contained in the original prompt,

56
00:03:59.201 --> 00:04:01.644
followed by the generated text.

57
00:04:01.644 --> 00:04:05.420
You can see that this model did a good
job of answering your question.

58
00:04:05.420 --> 00:04:10.845
It correctly identifies that Ganymede is a
moon of Jupiter and generates a reasonable

59
00:04:10.845 --> 00:04:16.234
answer to your question stating that the
moon is located within Jupiter's orbit.

60
00:04:16.234 --> 00:04:18.923
You'll see lots of examples of prompts and

61
00:04:18.923 --> 00:04:22.140
completions in this style
throughout the course.