WEBVTT

1
00:00:00.170 --> 00:00:04.922
Welcome back I'm here with my instructors
for this week, Mike and Shelby.

2
00:00:04.922 --> 00:00:10.105
Last week you learned about transformer
networks, which is really a key foundation

3
00:00:10.105 --> 00:00:14.713
for large language models, as well as
the Genitive AI project Life Cycle.

4
00:00:14.713 --> 00:00:17.784
And this week there's
lots more to dive into,

5
00:00:17.784 --> 00:00:21.901
starting with instruction tuning
of large language models.

6
00:00:21.901 --> 00:00:25.804
And then later how to carry out
fine-tuning in an efficient way.

7
00:00:25.804 --> 00:00:28.618
>> Yes, so
we take a look at instruction fine-tuning,

8
00:00:28.618 --> 00:00:32.620
so when you have your base model,
the thing that's initially pretrained,

9
00:00:32.620 --> 00:00:36.581
it's encoded a lot of really good
information, usually about the world.

10
00:00:36.581 --> 00:00:38.256
So it knows about things, but

11
00:00:38.256 --> 00:00:43.157
it doesn't necessarily know how to be able
to respond to our prompts, our questions.

12
00:00:43.157 --> 00:00:45.211
So when we instruct it
to do a certain task,

13
00:00:45.211 --> 00:00:47.445
it doesn't necessarily
know how to respond.

14
00:00:47.445 --> 00:00:52.162
And so instruction fine-tuning helps
it to be able to change its behavior

15
00:00:52.162 --> 00:00:53.843
to be more helpful for us.

16
00:00:53.843 --> 00:00:57.964
>> I thought instruction fine-tuning
was one of those major breakthroughs in

17
00:00:57.964 --> 00:01:00.484
the really history of
large language models.

18
00:01:00.484 --> 00:01:03.872
Because by learning off general text
off the Internet and other sources,

19
00:01:03.872 --> 00:01:05.518
you learn to predict the next word.

20
00:01:05.518 --> 00:01:09.214
By predicting what's the next word on
the Internet is not the same as following

21
00:01:09.214 --> 00:01:10.003
instructions.

22
00:01:10.003 --> 00:01:12.837
I thought it's amazing you can
take a large language model,

23
00:01:12.837 --> 00:01:15.626
train it on hundreds of billions
of words off the Internet.

24
00:01:15.626 --> 00:01:20.847
And then fine-tune it with a much smaller
data set on following instructions and

25
00:01:20.847 --> 00:01:22.317
just learn to do that.

26
00:01:22.317 --> 00:01:25.350
>> That's right and one of the things
you have to watch out for, of course,

27
00:01:25.350 --> 00:01:26.695
is catastrophic forgetting and

28
00:01:26.695 --> 00:01:28.855
this is something that we
talk about in the course.

29
00:01:28.855 --> 00:01:33.816
So that's where you train the model on
some extra data in this insane instruct

30
00:01:33.816 --> 00:01:34.893
fine-tuning.

31
00:01:34.893 --> 00:01:38.285
And then it forgets all of that
stuff that it had before, or

32
00:01:38.285 --> 00:01:40.790
a big chunk of that data
that it had before.

33
00:01:40.790 --> 00:01:43.459
And so there are some techniques
that we'll talk about in

34
00:01:43.459 --> 00:01:44.970
the course to help combat that.

35
00:01:44.970 --> 00:01:49.293
Such as doing instruct fine-tuning
across a really broad range of

36
00:01:49.293 --> 00:01:51.382
different instruction types.

37
00:01:51.382 --> 00:01:56.205
So it's not just a case of just tuning
it on just the thing you want it to do.

38
00:01:56.205 --> 00:01:58.864
You might have to be a little bit
broader than that as well, but

39
00:01:58.864 --> 00:02:00.203
we talk about it in the course.

40
00:02:00.203 --> 00:02:05.015
>> And so it turns out that there are two
types of fine-tuning that are very worth

41
00:02:05.015 --> 00:02:05.545
doing.

42
00:02:05.545 --> 00:02:08.300
One is that instruction fine-tuning
we just talked about, Mike.

43
00:02:08.300 --> 00:02:12.113
And then when a specific developer
is trying to fine-tune it for

44
00:02:12.113 --> 00:02:15.586
their own application, for
a specialized application.

45
00:02:15.586 --> 00:02:18.405
One of the problems with fine-tuning
is you take a giant model and

46
00:02:18.405 --> 00:02:20.773
you fine-tune every single
parameter in that model.

47
00:02:20.773 --> 00:02:23.747
You have this big thing to
store around and deploy, and

48
00:02:23.747 --> 00:02:26.601
it's actually very compute and
memory expansive.

49
00:02:26.601 --> 00:02:29.018
So fortunately,
there are better techniques than that.

50
00:02:29.018 --> 00:02:33.068
>> Right, and we talk about parameter
efficient fine-tuning or PEFT for short,

51
00:02:33.068 --> 00:02:37.191
as a set of methods that can allow you to
mitigate some of those concerns, right?

52
00:02:37.191 --> 00:02:40.615
So we have a lot of customers that
do want to be able to tune for

53
00:02:40.615 --> 00:02:43.367
very specific tasks,
very specific domains.

54
00:02:43.367 --> 00:02:48.512
And parameter efficient fine-tuning is
a great way to still achieve similar

55
00:02:48.512 --> 00:02:53.508
performance results on a lot of tasks
that you can with full fine-tuning.

56
00:02:53.508 --> 00:02:57.521
But then actually take advantage of
techniques that allow you to freeze those

57
00:02:57.521 --> 00:02:58.886
original model weights.

58
00:02:58.886 --> 00:03:03.734
Or add adaptive layers on top of that with
a much smaller memory footprint, right?

59
00:03:03.734 --> 00:03:06.389
So that you can train for multiple tasks.

60
00:03:06.389 --> 00:03:09.824
>> In fact, one of the techniques that
I know you've used a lot is LoRA.

61
00:03:09.824 --> 00:03:13.037
I remember when I read the LoRA paper,
I thought, this just makes sense,

62
00:03:13.037 --> 00:03:13.878
this is going to work.

63
00:03:13.878 --> 00:03:16.927
>> Right, we see a lot of
excitement demand around LoRA

64
00:03:16.927 --> 00:03:19.494
because of the performance
results of using

65
00:03:19.494 --> 00:03:23.398
those low rank matrices as opposed
to full fine-tuning, right?

66
00:03:23.398 --> 00:03:27.307
So you're able to get really good
performance results with minimal compute

67
00:03:27.307 --> 00:03:28.718
and memory requirements.

68
00:03:28.718 --> 00:03:31.368
>> So what I'm seeing among
different developers is many

69
00:03:31.368 --> 00:03:33.961
developers will often start
off with prompting, and

70
00:03:33.961 --> 00:03:37.377
sometimes that gives you good enough
performance and that's great.

71
00:03:37.377 --> 00:03:40.789
And sometimes prompting hits
a ceiling in performance and

72
00:03:40.789 --> 00:03:44.546
then this type of fine-tuning with LoRA or
other PEFT technique

73
00:03:44.546 --> 00:03:48.393
is really critical for
unlocking that extra level performance.

74
00:03:48.393 --> 00:03:52.766
And then the other thing I'm seeing
among a lot of OM developers

75
00:03:52.766 --> 00:03:56.975
is a discussion debate about
the cost of using a giant model,

76
00:03:56.975 --> 00:04:02.934
which is a lot of benefits versus for your
application fine-tuning a smaller model.

77
00:04:02.934 --> 00:04:06.638
>> Exactly, full fine tuning
can be cost prohibitive, right?

78
00:04:06.638 --> 00:04:07.818
To say the least so

79
00:04:07.818 --> 00:04:12.071
the ability to actually be able to
use techniques like PEFT to put

80
00:04:12.071 --> 00:04:17.057
fine-tuning generative AI models kind
of in the hands of everyday users.

81
00:04:17.057 --> 00:04:19.888
That do have those cost constraints and
they're cost conscious,

82
00:04:19.888 --> 00:04:22.326
which is pretty much everyone
in the real world, right?

83
00:04:22.326 --> 00:04:23.541
>> That's right and of course,

84
00:04:23.541 --> 00:04:26.026
if you're concerned about where
your data is going as well.

85
00:04:26.026 --> 00:04:28.653
So if it needs to be
running in your control,

86
00:04:28.653 --> 00:04:32.996
then having a model which is of
an appropriate size is really important.

87
00:04:32.996 --> 00:04:37.956
>> And so, once again, tons of
exciting stuff to dive into this week.

88
00:04:37.956 --> 00:04:41.841
Let's go on to the next video where Mike
will kick things off with instruction

89
00:04:41.841 --> 00:04:42.690
fine-tuning.