WEBVTT

1
00:00:00.000 --> 00:00:02.610
Throughout this
course, you've seen

2
00:00:02.610 --> 00:00:05.280
statements like the
model demonstrated

3
00:00:05.280 --> 00:00:08.055
good performance on this task or

4
00:00:08.055 --> 00:00:10.230
this fine-tuned model showed

5
00:00:10.230 --> 00:00:13.695
a large improvement in
performance over the base model.

6
00:00:13.695 --> 00:00:15.750
What do statements
like this mean?

7
00:00:15.750 --> 00:00:19.350
How can you formalize the
improvement in performance of

8
00:00:19.350 --> 00:00:21.180
your fine-tuned model over

9
00:00:21.180 --> 00:00:23.970
the pre-trained model
you started with?

10
00:00:23.970 --> 00:00:26.370
Let's explore several
metrics that are

11
00:00:26.370 --> 00:00:29.450
used by developers of large
language models that you

12
00:00:29.450 --> 00:00:31.610
can use to assess
the performance of

13
00:00:31.610 --> 00:00:33.890
your own models and compare

14
00:00:33.890 --> 00:00:36.485
to other models
out in the world.

15
00:00:36.485 --> 00:00:38.180
In traditional machine learning,

16
00:00:38.180 --> 00:00:40.010
you can assess how
well a model is

17
00:00:40.010 --> 00:00:42.530
doing by looking at
its performance on

18
00:00:42.530 --> 00:00:44.420
training and validation data

19
00:00:44.420 --> 00:00:47.710
sets where the output
is already known.

20
00:00:47.710 --> 00:00:51.605
You're able to calculate simple
metrics such as accuracy,

21
00:00:51.605 --> 00:00:54.500
which states the fraction
of all predictions that are

22
00:00:54.500 --> 00:00:58.630
correct because the
models are deterministic.

23
00:00:58.630 --> 00:01:02.225
But with large language
models where the output is

24
00:01:02.225 --> 00:01:05.975
non-deterministic and
language-based evaluation

25
00:01:05.975 --> 00:01:08.245
is much more challenging.

26
00:01:08.245 --> 00:01:10.535
Take, for example, the sentence,

27
00:01:10.535 --> 00:01:13.150
Mike really loves drinking tea.

28
00:01:13.150 --> 00:01:17.540
This is quite similar to
Mike adores sipping tea.

29
00:01:17.540 --> 00:01:20.365
But how do you measure
the similarity?

30
00:01:20.365 --> 00:01:23.730
Let's look at these
other two sentences.

31
00:01:23.730 --> 00:01:26.045
Mike does not drink coffee,

32
00:01:26.045 --> 00:01:29.185
and Mike does drink coffee.

33
00:01:29.185 --> 00:01:31.730
There is only one
word difference

34
00:01:31.730 --> 00:01:33.635
between these two sentences.

35
00:01:33.635 --> 00:01:37.610
However, the meaning is
completely different.

36
00:01:37.610 --> 00:01:42.320
Now, for humans like us with
squishy organic brains,

37
00:01:42.320 --> 00:01:45.175
we can see the similarities
and differences.

38
00:01:45.175 --> 00:01:48.620
But when you train a model
on millions of sentences,

39
00:01:48.620 --> 00:01:50.390
you need an automated,

40
00:01:50.390 --> 00:01:53.345
structured way to
make measurements.

41
00:01:53.345 --> 00:01:55.490
ROUGE and BLEU,

42
00:01:55.490 --> 00:01:58.400
are two widely used
evaluation metrics

43
00:01:58.400 --> 00:02:00.175
for different tasks.

44
00:02:00.175 --> 00:02:03.740
ROUGE or recall oriented under

45
00:02:03.740 --> 00:02:07.490
study for jesting
evaluation is primarily

46
00:02:07.490 --> 00:02:09.845
employed to assess
the quality of

47
00:02:09.845 --> 00:02:12.440
automatically
generated summaries by

48
00:02:12.440 --> 00:02:16.070
comparing them to human-generated
reference summaries.

49
00:02:16.070 --> 00:02:18.055
On the other hand, BLEU,

50
00:02:18.055 --> 00:02:21.680
or bilingual evaluation
understudy is

51
00:02:21.680 --> 00:02:23.945
an algorithm
designed to evaluate

52
00:02:23.945 --> 00:02:26.855
the quality of
machine-translated text,

53
00:02:26.855 --> 00:02:31.635
again, by comparing it to
human-generated translations.

54
00:02:31.635 --> 00:02:35.685
Now the word BLEU
is French for blue.

55
00:02:35.685 --> 00:02:38.655
You might hear people
calling this blue

56
00:02:38.655 --> 00:02:42.365
but here I'm going to stick
with the original BLEU.

57
00:02:42.365 --> 00:02:44.855
Before we start
calculating metrics.

58
00:02:44.855 --> 00:02:47.170
Let's review some terminology.

59
00:02:47.170 --> 00:02:48.990
In the anatomy of language,

60
00:02:48.990 --> 00:02:52.985
a unigram is equivalent
to a single word.

61
00:02:52.985 --> 00:03:00.080
A bigram is two words and
n-gram is a group of n-words.

62
00:03:00.080 --> 00:03:01.720
Pretty straightforward stuff.

63
00:03:01.720 --> 00:03:05.730
First, let's look at
the ROUGE-1 metric.

64
00:03:05.730 --> 00:03:07.580
To do so, let's look at

65
00:03:07.580 --> 00:03:10.250
a human-generated
reference sentence.

66
00:03:10.250 --> 00:03:12.680
It is cold outside and

67
00:03:12.680 --> 00:03:16.500
a generated output that
is very cold outside.

68
00:03:16.500 --> 00:03:20.090
You can perform simple
metric calculations similar

69
00:03:20.090 --> 00:03:23.600
to other machine-learning
tasks using recall,

70
00:03:23.600 --> 00:03:25.630
precision, and F1.

71
00:03:25.630 --> 00:03:28.280
The recall metric
measures the number

72
00:03:28.280 --> 00:03:31.070
of words or unigrams
that are matched

73
00:03:31.070 --> 00:03:34.115
between the reference
and the generated output

74
00:03:34.115 --> 00:03:36.230
divided by the
number of words or

75
00:03:36.230 --> 00:03:38.575
unigrams in the reference.

76
00:03:38.575 --> 00:03:42.530
In this case, that gets a
perfect score of one as

77
00:03:42.530 --> 00:03:46.790
all the generated words match
words in the reference.

78
00:03:46.790 --> 00:03:49.880
Precision measures
the unigram matches

79
00:03:49.880 --> 00:03:52.660
divided by the output size.

80
00:03:52.660 --> 00:03:54.980
The F1 score is

81
00:03:54.980 --> 00:03:58.055
the harmonic mean of
both of these values.

82
00:03:58.055 --> 00:04:00.500
These are very basic metrics

83
00:04:00.500 --> 00:04:02.930
that only focused on
individual words,

84
00:04:02.930 --> 00:04:05.240
hence the one in the name,

85
00:04:05.240 --> 00:04:07.955
and don't consider the
ordering of the words.

86
00:04:07.955 --> 00:04:09.625
It can be deceptive.

87
00:04:09.625 --> 00:04:12.560
It's easily possible to
generate sentences that

88
00:04:12.560 --> 00:04:15.935
score well but would
be subjectively poor.

89
00:04:15.935 --> 00:04:19.370
Stop for a moment and imagine
that the sentence generated

90
00:04:19.370 --> 00:04:22.625
by the model was different
by just one word.

91
00:04:22.625 --> 00:04:26.900
Not, so it is not cold outside.

92
00:04:26.900 --> 00:04:29.405
The scores would be the same.

93
00:04:29.405 --> 00:04:31.610
You can get a slightly
better score by

94
00:04:31.610 --> 00:04:33.950
taking into account bigrams or

95
00:04:33.950 --> 00:04:36.380
collections of two
words at a time from

96
00:04:36.380 --> 00:04:39.040
the reference and
generated sentence.

97
00:04:39.040 --> 00:04:41.225
By working with pairs of words

98
00:04:41.225 --> 00:04:43.730
you're acknowledging
in a very simple way,

99
00:04:43.730 --> 00:04:46.790
the ordering of the
words in the sentence.

100
00:04:46.790 --> 00:04:51.410
By using bigrams, you're
able to calculate a ROUGE-2.

101
00:04:51.410 --> 00:04:54.680
Now, you can calculate
the recall, precision,

102
00:04:54.680 --> 00:04:56.525
and F1 score using

103
00:04:56.525 --> 00:05:00.155
bigram matches instead
of individual words.

104
00:05:00.155 --> 00:05:02.360
You'll notice that
the scores are

105
00:05:02.360 --> 00:05:04.985
lower than the ROUGE-1 scores.

106
00:05:04.985 --> 00:05:07.070
With longer sentences, they're

107
00:05:07.070 --> 00:05:09.905
a greater chance that
bigrams don't match,

108
00:05:09.905 --> 00:05:12.425
and the scores may
be even lower.

109
00:05:12.425 --> 00:05:15.660
Rather than continue
on with ROUGE numbers

110
00:05:15.660 --> 00:05:19.410
growing bigger to n-grams
of three or fours,

111
00:05:19.410 --> 00:05:21.505
let's take a different approach.

112
00:05:21.505 --> 00:05:23.120
Instead, you'll look for

113
00:05:23.120 --> 00:05:26.210
the longest common
subsequence present in

114
00:05:26.210 --> 00:05:29.770
both the generated output
and the reference output.

115
00:05:29.770 --> 00:05:33.725
In this case, the longest
matching sub-sequences are,

116
00:05:33.725 --> 00:05:36.935
it is and cold outside,

117
00:05:36.935 --> 00:05:38.900
each with a length of two.

118
00:05:38.900 --> 00:05:41.990
You can now use the LCS value to

119
00:05:41.990 --> 00:05:45.575
calculate the recall
precision and F1 score,

120
00:05:45.575 --> 00:05:49.460
where the numerator in both
the recall and precision

121
00:05:49.460 --> 00:05:51.620
calculations is the length

122
00:05:51.620 --> 00:05:53.900
of the longest
common subsequence,

123
00:05:53.900 --> 00:05:56.050
in this case, two.

124
00:05:56.050 --> 00:05:59.410
Collectively, these
three quantities

125
00:05:59.410 --> 00:06:02.990
are known as the Rouge-L score.

126
00:06:02.990 --> 00:06:05.260
As with all of the rouge scores,

127
00:06:05.260 --> 00:06:07.950
you need to take the
values in context.

128
00:06:07.950 --> 00:06:09.640
You can only use the scores

129
00:06:09.640 --> 00:06:11.200
to compare the capabilities of

130
00:06:11.200 --> 00:06:15.295
models if the scores were
determined for the same task.

131
00:06:15.295 --> 00:06:17.550
For example, summarization.

132
00:06:17.550 --> 00:06:20.090
Rouge scores for different tasks

133
00:06:20.090 --> 00:06:22.355
are not comparable
to one another.

134
00:06:22.355 --> 00:06:24.880
As you've seen, a
particular problem

135
00:06:24.880 --> 00:06:27.095
with simple rouge scores is

136
00:06:27.095 --> 00:06:28.270
that it's possible for

137
00:06:28.270 --> 00:06:31.715
a bad completion to
result in a good score.

138
00:06:31.715 --> 00:06:34.690
Take, for example,
this generated output,

139
00:06:34.690 --> 00:06:37.830
cold, cold, cold, cold.

140
00:06:37.830 --> 00:06:40.370
As this generated
output contains

141
00:06:40.370 --> 00:06:42.985
one of the words from
the reference sentence,

142
00:06:42.985 --> 00:06:45.055
it will score quite highly,

143
00:06:45.055 --> 00:06:49.045
even though the same word
is repeated multiple times.

144
00:06:49.045 --> 00:06:52.925
The Rouge-1 precision
score will be perfect.

145
00:06:52.925 --> 00:06:55.570
One way you can counter
this issue is by

146
00:06:55.570 --> 00:06:58.480
using a clipping function
to limit the number of

147
00:06:58.480 --> 00:07:01.505
unigram matches to
the maximum count

148
00:07:01.505 --> 00:07:04.205
for that unigram
within the reference.

149
00:07:04.205 --> 00:07:05.675
In this case, there is

150
00:07:05.675 --> 00:07:08.590
one appearance of cold
and the reference and

151
00:07:08.590 --> 00:07:11.130
so a modified precision
with a clip on

152
00:07:11.130 --> 00:07:13.360
the unigram matches results

153
00:07:13.360 --> 00:07:15.790
in a dramatically reduced score.

154
00:07:15.790 --> 00:07:18.060
However, you'll still
be challenged if

155
00:07:18.060 --> 00:07:20.305
their generated words
are all present,

156
00:07:20.305 --> 00:07:22.510
but just in a different order.

157
00:07:22.510 --> 00:07:25.550
For example, with this
generated sentence,

158
00:07:25.550 --> 00:07:27.970
outside cold it is.

159
00:07:27.970 --> 00:07:29.470
This sentence was called

160
00:07:29.470 --> 00:07:32.320
perfectly even on the
modified precision with

161
00:07:32.320 --> 00:07:34.865
the clipping function
as all of the words and

162
00:07:34.865 --> 00:07:38.725
the generated output are
present in the reference.

163
00:07:38.725 --> 00:07:41.530
Whilst using a
different rouge score

164
00:07:41.530 --> 00:07:44.030
can help experimenting with

165
00:07:44.030 --> 00:07:46.090
a n-gram size that
will calculate

166
00:07:46.090 --> 00:07:50.100
the most useful score will be
dependent on the sentence,

167
00:07:50.100 --> 00:07:53.270
the sentence size,
and your use case.

168
00:07:53.270 --> 00:07:56.710
Note that many language model
libraries, for example,

169
00:07:56.710 --> 00:08:00.245
Hugging Face, which you used
in the first week's lab,

170
00:08:00.245 --> 00:08:03.690
include implementations
of rouge score that you

171
00:08:03.690 --> 00:08:07.670
can use to easily evaluate
the output of your model.

172
00:08:07.670 --> 00:08:11.800
You'll get to try the rouge
score and use it to compare

173
00:08:11.800 --> 00:08:14.135
the model's
performance before and

174
00:08:14.135 --> 00:08:17.005
after fine-tuning
in this week's lab.

175
00:08:17.005 --> 00:08:19.780
The other score that can
be useful in evaluating

176
00:08:19.780 --> 00:08:23.225
the performance of your
model is the BLEU score,

177
00:08:23.225 --> 00:08:27.545
which stands for bilingual
evaluation under study.

178
00:08:27.545 --> 00:08:31.110
Just to remind you that
BLEU score is useful for

179
00:08:31.110 --> 00:08:35.450
evaluating the quality of
machine-translated text.

180
00:08:35.450 --> 00:08:37.930
The score itself is
calculated using

181
00:08:37.930 --> 00:08:42.410
the average precision over
multiple n-gram sizes.

182
00:08:42.410 --> 00:08:46.565
Just like the Rouge-1 score
that we looked at before,

183
00:08:46.565 --> 00:08:49.020
but calculated for a range of

184
00:08:49.020 --> 00:08:51.680
n-gram sizes and then averaged.

185
00:08:51.680 --> 00:08:54.100
Let's take a closer
look at what this

186
00:08:54.100 --> 00:08:56.640
measures and how
it's calculated.

187
00:08:56.640 --> 00:08:59.315
The BLEU score quantifies

188
00:08:59.315 --> 00:09:01.980
the quality of a
translation by checking

189
00:09:01.980 --> 00:09:03.670
how many n-grams in

190
00:09:03.670 --> 00:09:06.430
the machine-generated
translation match

191
00:09:06.430 --> 00:09:09.185
those in the reference
translation.

192
00:09:09.185 --> 00:09:11.015
To calculate the score,

193
00:09:11.015 --> 00:09:12.995
you average precision across

194
00:09:12.995 --> 00:09:16.110
a range of different
n-gram sizes.

195
00:09:16.110 --> 00:09:18.490
If you were to
calculate this by hand,

196
00:09:18.490 --> 00:09:21.430
you would carry out multiple
calculations and then

197
00:09:21.430 --> 00:09:25.060
average all of the results
to find the BLEU score.

198
00:09:25.060 --> 00:09:27.460
For this example,
let's take a look at

199
00:09:27.460 --> 00:09:29.600
a longer sentence
so that you can get

200
00:09:29.600 --> 00:09:32.170
a better sense of
the scores value.

201
00:09:32.170 --> 00:09:35.225
The reference
human-provided sentence is,

202
00:09:35.225 --> 00:09:37.650
I am very happy to say that I

203
00:09:37.650 --> 00:09:40.515
am drinking a warm cup of tea.

204
00:09:40.515 --> 00:09:42.130
Now, as you've seen

205
00:09:42.130 --> 00:09:43.985
these individual calculations in

206
00:09:43.985 --> 00:09:46.300
depth when you looked at rouge,

207
00:09:46.300 --> 00:09:47.960
I will show you the results of

208
00:09:47.960 --> 00:09:50.630
BLEU using a standard library.

209
00:09:50.630 --> 00:09:52.925
Calculating the BLEU score is

210
00:09:52.925 --> 00:09:55.720
easy with pre-written
libraries from providers

211
00:09:55.720 --> 00:09:58.020
like Hugging Face
and I've done just

212
00:09:58.020 --> 00:10:00.805
that for each of our
candidate sentences.

213
00:10:00.805 --> 00:10:02.810
The first candidate is,

214
00:10:02.810 --> 00:10:07.115
I am very happy that I am
drinking a cup of tea.

215
00:10:07.115 --> 00:10:11.860
The BLEU score is 0.495.

216
00:10:11.860 --> 00:10:15.980
As we get closer and closer
to the original sentence,

217
00:10:15.980 --> 00:10:20.140
we get a score that is
closer and closer to one.

218
00:10:20.140 --> 00:10:22.170
Both rouge and BLEU are

219
00:10:22.170 --> 00:10:24.260
quite simple metrics and are

220
00:10:24.260 --> 00:10:26.800
relatively low-cost
to calculate.

221
00:10:26.800 --> 00:10:28.180
You can use them for

222
00:10:28.180 --> 00:10:31.395
simple reference as you
iterate over your models,

223
00:10:31.395 --> 00:10:33.930
but you shouldn't use
them alone to report

224
00:10:33.930 --> 00:10:37.265
the final evaluation of
a large language model.

225
00:10:37.265 --> 00:10:41.060
Use rouge for diagnostic
evaluation of

226
00:10:41.060 --> 00:10:45.615
summarization tasks and
BLEU for translation tasks.

227
00:10:45.615 --> 00:10:47.680
For overall evaluation of

228
00:10:47.680 --> 00:10:49.735
your model's
performance, however,

229
00:10:49.735 --> 00:10:51.340
you will need to look at one of

230
00:10:51.340 --> 00:10:53.230
the evaluation
benchmarks that have

231
00:10:53.230 --> 00:10:55.370
been developed by researchers.

232
00:10:55.370 --> 00:10:57.305
Let's take a look
at some of these in

233
00:10:57.305 --> 00:11:00.400
more detail in the next video.