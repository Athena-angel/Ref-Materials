WEBVTT

1
00:00:00.000 --> 00:00:02.010
This week's lab, let you try out

2
00:00:02.010 --> 00:00:04.110
fine-tuning using
PEFT with LoRA for

3
00:00:04.110 --> 00:00:07.170
yourself by improving the
summarization ability

4
00:00:07.170 --> 00:00:09.195
of the Flan-T5 model.

5
00:00:09.195 --> 00:00:10.800
My colleague Chris, is going to

6
00:00:10.800 --> 00:00:12.825
walk you through this
week's notebook.

7
00:00:12.825 --> 00:00:14.385
I'll pass you over to him.

8
00:00:14.385 --> 00:00:15.735
Hey, thanks, Shelby.

9
00:00:15.735 --> 00:00:17.790
Now, let's take a look at Lab 2.

10
00:00:17.790 --> 00:00:20.295
In Lab 2, you will
get hands-on with

11
00:00:20.295 --> 00:00:25.110
full fine-tuning and
Parameter-Efficient Fine-Tuning,

12
00:00:25.110 --> 00:00:29.115
also called PEFT with
prompt instructions.

13
00:00:29.115 --> 00:00:33.255
You will tune the Flan-T5
model further with

14
00:00:33.255 --> 00:00:35.685
your own specific prompts

15
00:00:35.685 --> 00:00:39.120
for your specific
summarization task.

16
00:00:39.120 --> 00:00:41.700
Let's jump right
into the notebook.

17
00:00:41.700 --> 00:00:47.400
Lab 2, we are going to
actually fine-tune a model.

18
00:00:47.400 --> 00:00:49.640
Lab 1, we were doing

19
00:00:49.640 --> 00:00:52.715
the zero-shot inference,
the in-context learning.

20
00:00:52.715 --> 00:00:55.490
Now we are actually
going to modify

21
00:00:55.490 --> 00:00:59.495
the weights of our
language model,

22
00:00:59.495 --> 00:01:02.525
specific to our
summarization task

23
00:01:02.525 --> 00:01:05.105
and specific to our dataset.

24
00:01:05.105 --> 00:01:07.190
Real quick, just double-check

25
00:01:07.190 --> 00:01:08.780
that you have the eight CPU,

26
00:01:08.780 --> 00:01:11.600
32 gigabyte, that's the
instance type here.

27
00:01:11.600 --> 00:01:16.520
This is an AWS instance type
from SageMaker, ml.m5.2xl.

28
00:01:16.520 --> 00:01:18.160
Let's do this pip installs.

29
00:01:18.160 --> 00:01:19.770
While the pip installs
are happening,

30
00:01:19.770 --> 00:01:23.250
let me explain torch and
torchdata the same as Lab

31
00:01:23.250 --> 00:01:26.940
1 where we are going
to use PyTorch,

32
00:01:26.940 --> 00:01:29.220
we are then pip installing

33
00:01:29.220 --> 00:01:30.600
the torchdata library to

34
00:01:30.600 --> 00:01:32.565
help with the PyTorch
data loading.

35
00:01:32.565 --> 00:01:35.765
There's also a library
called evaluates,

36
00:01:35.765 --> 00:01:37.580
and this is what we're
going to use with

37
00:01:37.580 --> 00:01:40.610
our rouge score to
calculate rouge.

38
00:01:40.610 --> 00:01:43.715
You learned about rouge in

39
00:01:43.715 --> 00:01:48.230
the lessons as a way
to measure how well

40
00:01:48.230 --> 00:01:51.050
does a summary
encapsulate what was in

41
00:01:51.050 --> 00:01:56.275
the original conversation
or the original text.

42
00:01:56.275 --> 00:01:58.140
Now, these two libraries,

43
00:01:58.140 --> 00:01:59.460
LoRA and PEFT,

44
00:01:59.460 --> 00:02:02.805
you heard about a
bit in the lessons.

45
00:02:02.805 --> 00:02:05.270
This is what we will use to

46
00:02:05.270 --> 00:02:08.170
do the parameter
efficient fine-tuning.

47
00:02:08.170 --> 00:02:09.920
Now, I'm going to do

48
00:02:09.920 --> 00:02:13.310
some imports here from
those pip installs.

49
00:02:13.310 --> 00:02:15.170
If you do see this,

50
00:02:15.170 --> 00:02:17.045
sometimes this clean data

51
00:02:17.045 --> 00:02:18.875
in minutes thing shows up here,

52
00:02:18.875 --> 00:02:21.185
you don't need this for the lab.

53
00:02:21.185 --> 00:02:23.360
If you see it, I think this

54
00:02:23.360 --> 00:02:25.850
comes up whenever
we import pandas,

55
00:02:25.850 --> 00:02:28.790
just click the "X"
and click "Don't Show

56
00:02:28.790 --> 00:02:32.135
Again" because we're not
using that part of SageMaker.

57
00:02:32.135 --> 00:02:35.075
Once again, we have the
AutoModelForSeq2Seq.

58
00:02:35.075 --> 00:02:36.590
This is what's going
to give us access to

59
00:02:36.590 --> 00:02:39.920
Flan-T5 through the
transformers python library,

60
00:02:39.920 --> 00:02:42.110
the tokenizer, we use

61
00:02:42.110 --> 00:02:44.465
generation config in
the previous lab.

62
00:02:44.465 --> 00:02:47.030
Now we're going to
see two new classes,

63
00:02:47.030 --> 00:02:49.010
one called TrainingArguments,
one called Trainer.

64
00:02:49.010 --> 00:02:51.215
These are all from transformers,

65
00:02:51.215 --> 00:02:56.030
these are always we can use
that simplifies our code when

66
00:02:56.030 --> 00:02:58.250
we're trying to train
our language model

67
00:02:58.250 --> 00:03:00.985
or fine-tune our language model.

68
00:03:00.985 --> 00:03:02.630
We see that we are going to

69
00:03:02.630 --> 00:03:05.525
import PyTorch and the evaluate,

70
00:03:05.525 --> 00:03:10.085
and we will use I believe
pandas and numpy later on.

71
00:03:10.085 --> 00:03:14.340
Let's load the dataset just
like we did in the first lab.

72
00:03:15.260 --> 00:03:18.020
Let's load the model
just like we did in

73
00:03:18.020 --> 00:03:19.940
the first lab and the tokenizer,

74
00:03:19.940 --> 00:03:21.200
and this is called

75
00:03:21.200 --> 00:03:26.000
the original model and this
will be useful later when we

76
00:03:26.000 --> 00:03:29.300
compare all the different
fine-tuning strategies

77
00:03:29.300 --> 00:03:32.365
to the original model
that is not fine-tuned.

78
00:03:32.365 --> 00:03:34.250
Here is a convenience
function that

79
00:03:34.250 --> 00:03:36.020
prints out all of the parameters

80
00:03:36.020 --> 00:03:37.100
that are in the model and

81
00:03:37.100 --> 00:03:39.770
specifically the
trainable parameters.

82
00:03:39.770 --> 00:03:44.060
This will become useful
when we introduce

83
00:03:44.060 --> 00:03:46.340
the PEFT version of
the model which does

84
00:03:46.340 --> 00:03:49.220
not train all of the parameters.

85
00:03:49.220 --> 00:03:50.525
Here we see there are

86
00:03:50.525 --> 00:03:54.965
approximately 250
million parameters

87
00:03:54.965 --> 00:03:58.400
being trained when we do
the full fine-tuning,

88
00:03:58.400 --> 00:03:59.570
which is the first part of

89
00:03:59.570 --> 00:04:00.900
this lab where we
full fine-tune.

90
00:04:00.900 --> 00:04:03.530
The second part of the
lab will be where we do

91
00:04:03.530 --> 00:04:05.765
the parameter
efficient fine-tuning

92
00:04:05.765 --> 00:04:07.340
specifically with LoRA,

93
00:04:07.340 --> 00:04:09.680
where we will only train
very small number.

94
00:04:09.680 --> 00:04:10.855
So keep that in mind,

95
00:04:10.855 --> 00:04:12.830
this is a kind of a
lot of messy code but

96
00:04:12.830 --> 00:04:16.055
it's pretty useful
for the comparison.

97
00:04:16.055 --> 00:04:19.010
Just like we did
in the first lab,

98
00:04:19.010 --> 00:04:21.140
we're going to show
a sample input.

99
00:04:21.140 --> 00:04:22.895
We're going to show
the human baseline.

100
00:04:22.895 --> 00:04:25.160
We're going to do the zero shot.

101
00:04:25.160 --> 00:04:27.470
This is not one shot,

102
00:04:27.470 --> 00:04:30.740
not few shot, we're pass
that, that was Lab 1.

103
00:04:30.740 --> 00:04:34.385
Here, we are trying to
gets the point where

104
00:04:34.385 --> 00:04:38.330
one simple call
into our model can

105
00:04:38.330 --> 00:04:41.690
give us a decent summary without

106
00:04:41.690 --> 00:04:43.670
having to pass in
the one shot and

107
00:04:43.670 --> 00:04:46.315
few shot examples,
that's the goal.

108
00:04:46.315 --> 00:04:49.310
The first way that we're
going to do is we are going

109
00:04:49.310 --> 00:04:53.555
to do full fine-tuning.

110
00:04:53.555 --> 00:04:57.140
Here is a convenience
function that can tokenize

111
00:04:57.140 --> 00:05:01.825
and wrap our dataset
in a prompt.

112
00:05:01.825 --> 00:05:04.380
As we saw in the first
lab where we had

113
00:05:04.380 --> 00:05:07.295
a prompt that said summarize
the following conversation,

114
00:05:07.295 --> 00:05:09.860
and then we're actually going
to give it the dialogue,

115
00:05:09.860 --> 00:05:11.360
and then we're going
to end the prompt

116
00:05:11.360 --> 00:05:13.525
with those summary colon.

117
00:05:13.525 --> 00:05:17.450
This function will let us map
over all of the elements of

118
00:05:17.450 --> 00:05:20.510
our dataset and convert

119
00:05:20.510 --> 00:05:23.715
them into prompts
with instruction.

120
00:05:23.715 --> 00:05:25.520
That's what we're
going to do here,

121
00:05:25.520 --> 00:05:30.930
which is full fine-tuning
with instruction prompts.

122
00:05:31.010 --> 00:05:34.550
Here, we're just going to
take a sample just to keep

123
00:05:34.550 --> 00:05:38.760
the resource requirements
law for this particular lab,

124
00:05:38.760 --> 00:05:40.410
speed things up a little bit.

125
00:05:40.410 --> 00:05:42.285
Let's take a look at the size.

126
00:05:42.285 --> 00:05:46.715
Here we have about 125
training examples.

127
00:05:46.715 --> 00:05:48.855
We're going to use
five for validation.

128
00:05:48.855 --> 00:05:51.770
We're going to use
15 to actually do

129
00:05:51.770 --> 00:05:55.780
our holdout test later
on when we compare.

130
00:05:55.780 --> 00:05:57.460
We're going to fine tune with

131
00:05:57.460 --> 00:05:58.780
the training and we're going to

132
00:05:58.780 --> 00:06:00.655
validate with the validation.

133
00:06:00.655 --> 00:06:02.630
Then when all of
that said and done,

134
00:06:02.630 --> 00:06:05.770
we're then going to use the
15 test examples to then

135
00:06:05.770 --> 00:06:08.680
compare the different strategies

136
00:06:08.680 --> 00:06:10.910
for fine-tuning
with instruction.

137
00:06:10.910 --> 00:06:13.240
Here we see training
arguments and we

138
00:06:13.240 --> 00:06:15.370
see some defaults here
for the learning rate.

139
00:06:15.370 --> 00:06:17.110
We see some pretty
low values for

140
00:06:17.110 --> 00:06:20.330
the max steps and the
number of the epochs.

141
00:06:20.330 --> 00:06:23.200
That's because we
do want to try to

142
00:06:23.200 --> 00:06:24.485
minimize the amount of

143
00:06:24.485 --> 00:06:26.725
compute that's
needed for this lab.

144
00:06:26.725 --> 00:06:28.345
If you have more time,

145
00:06:28.345 --> 00:06:29.495
you can certainly change

146
00:06:29.495 --> 00:06:32.950
these values and bump them
up to maybe five epochs,

147
00:06:32.950 --> 00:06:35.350
maybe max steps 100.

148
00:06:35.350 --> 00:06:36.935
In a bit, I'll show

149
00:06:36.935 --> 00:06:38.870
you how we actually
work around that.

150
00:06:38.870 --> 00:06:42.795
We have trained offline
a much larger model with

151
00:06:42.795 --> 00:06:47.050
much higher max steps and
training epochs and in a bit,

152
00:06:47.050 --> 00:06:48.245
we will actually pull that

153
00:06:48.245 --> 00:06:50.110
in and then continue from there.

154
00:06:50.110 --> 00:06:52.150
But this is what the
code looks like.

155
00:06:52.150 --> 00:06:53.470
Here's the training dataset,

156
00:06:53.470 --> 00:06:56.425
there's the evaluation
validation dataset,

157
00:06:56.425 --> 00:06:58.000
here's where we call train.

158
00:06:58.000 --> 00:07:02.290
Actually let me just do Shift
Enter, get this started.

159
00:07:02.290 --> 00:07:03.920
This will take a few minutes,

160
00:07:03.920 --> 00:07:07.025
even with the low max
steps and the low epochs,

161
00:07:07.025 --> 00:07:09.980
this still does take
a few minutes to run.

162
00:07:09.980 --> 00:07:13.350
Then here's that step
where we actually pull

163
00:07:13.350 --> 00:07:17.980
in from the cloud
objects storage,

164
00:07:17.980 --> 00:07:20.280
a model that we
trained outside of

165
00:07:20.280 --> 00:07:22.980
this lab that is a
little bit better.

166
00:07:22.980 --> 00:07:25.065
So we'll actually
start with that.

167
00:07:25.065 --> 00:07:28.555
Let's give the train a
few minutes to complete.

168
00:07:28.555 --> 00:07:30.610
What we're doing here is we

169
00:07:30.610 --> 00:07:33.665
are actually
instruction fine-tuning

170
00:07:33.665 --> 00:07:36.535
our Flan-T5 language model with

171
00:07:36.535 --> 00:07:38.800
our specific dataset on

172
00:07:38.800 --> 00:07:41.275
a very specific
summarization task.

173
00:07:41.275 --> 00:07:42.460
Then later we'll see how

174
00:07:42.460 --> 00:07:44.890
the ROUGE metric
compares between

175
00:07:44.890 --> 00:07:46.240
the original model and

176
00:07:46.240 --> 00:07:49.715
the instruction fine-tune
model that we have here.

177
00:07:49.715 --> 00:07:53.065
Let's pull in that model from

178
00:07:53.065 --> 00:07:56.200
S3 object storage that we
trained offline that's

179
00:07:56.200 --> 00:07:59.530
a little bit better accuracy and

180
00:07:59.530 --> 00:08:02.510
lower loss that we were able to

181
00:08:02.510 --> 00:08:06.430
train for longer outside
of this specific lab.

182
00:08:06.430 --> 00:08:09.480
I do want to keep an eye
on the size of this model.

183
00:08:09.480 --> 00:08:13.125
This is a fully fine-tuned
instruction model,

184
00:08:13.125 --> 00:08:15.575
and you'll see it's
close to one gigabyte,

185
00:08:15.575 --> 00:08:17.020
and that will come in handy

186
00:08:17.020 --> 00:08:19.420
later when we
compare it to PEFT,

187
00:08:19.420 --> 00:08:21.475
which is on the order
of 10 megabytes.

188
00:08:21.475 --> 00:08:23.990
Here we see 945 megabytes,

189
00:08:23.990 --> 00:08:27.380
so we pulled that
model down into

190
00:08:27.380 --> 00:08:31.315
a directory here called flan
dialogue summary checkpoint.

191
00:08:31.315 --> 00:08:33.820
Now we're going to load
that instruction model,

192
00:08:33.820 --> 00:08:36.910
so now this becomes
our new model that we

193
00:08:36.910 --> 00:08:40.355
are then going to use to
compare here in a bit.

194
00:08:40.355 --> 00:08:43.455
Now that we've loaded what
we're calling instruct model,

195
00:08:43.455 --> 00:08:45.335
let's actually try from

196
00:08:45.335 --> 00:08:48.410
our test dataset
using the human eye,

197
00:08:48.410 --> 00:08:52.200
let's qualitatively test
and see how does this look.

198
00:08:52.200 --> 00:08:55.060
The baseline summary Person
1 teaches Person 2 how to

199
00:08:55.060 --> 00:08:58.445
upgrade in Person 2's system.

200
00:08:58.445 --> 00:09:00.885
The original model without

201
00:09:00.885 --> 00:09:04.055
any instruction fine-tuning,
just zero-shot.

202
00:09:04.055 --> 00:09:06.630
This time it's
giving us Person 1,

203
00:09:06.630 --> 00:09:09.120
you'd like to upgrade
your computer, Person 2,

204
00:09:09.120 --> 00:09:12.580
you'd like to upgrade your
computer, so not very good.

205
00:09:12.580 --> 00:09:15.605
The instruction fine tune
model that we just got done

206
00:09:15.605 --> 00:09:18.335
training is Person 1

207
00:09:18.335 --> 00:09:21.215
suggests Person 2 should
upgrade their system,

208
00:09:21.215 --> 00:09:22.390
hardware, and CD ROM,

209
00:09:22.390 --> 00:09:24.340
Person 2 thinks
that's a great idea.

210
00:09:24.340 --> 00:09:27.095
That's qualitatively,
that's just looking.

211
00:09:27.095 --> 00:09:30.145
Now, we only took a
look at one example,

212
00:09:30.145 --> 00:09:31.680
but this is why we

213
00:09:31.680 --> 00:09:35.890
have quantitative techniques
to do this comparison,

214
00:09:35.890 --> 00:09:37.260
to do the evaluation.

215
00:09:37.260 --> 00:09:39.155
Specifically, let's load

216
00:09:39.155 --> 00:09:42.365
ROUGE and we're going
to take a look,

217
00:09:42.365 --> 00:09:45.655
I think we're just going to
do maybe the first 10 here,

218
00:09:45.655 --> 00:09:47.425
and let's compare them.

219
00:09:47.425 --> 00:09:50.770
Let's take the first 10
from our test dataset.

220
00:09:50.770 --> 00:09:54.160
We will run them through
these conversations,

221
00:09:54.160 --> 00:09:57.305
through both the original
Flan-T5 model as well as

222
00:09:57.305 --> 00:10:00.380
the instruction fine-tuned model

223
00:10:00.380 --> 00:10:02.950
that we trained up above.

224
00:10:02.950 --> 00:10:04.905
Here, of course, we're
going to wrap it in a

225
00:10:04.905 --> 00:10:07.995
prompt similar to what
we used to train.

226
00:10:07.995 --> 00:10:10.715
Then let's see how it did.

227
00:10:10.715 --> 00:10:15.280
This is qualitatively taking
a look at them side-by-side.

228
00:10:15.280 --> 00:10:18.790
Let's compare the
ROUGE metrics for

229
00:10:18.790 --> 00:10:20.880
both the original Flan-T5 and

230
00:10:20.880 --> 00:10:23.960
the instruction fine-tune
model that we tuned up above.

231
00:10:23.960 --> 00:10:26.795
Here we see that the instruction
fine-tuned model score

232
00:10:26.795 --> 00:10:28.090
is much higher on

233
00:10:28.090 --> 00:10:30.490
the ROUGE evaluation metric

234
00:10:30.490 --> 00:10:32.730
than the original Flan-T5 model.

235
00:10:32.730 --> 00:10:34.930
This is showing that
with a little bit of

236
00:10:34.930 --> 00:10:38.665
fine-tuning using our dataset
and a specific prompt,

237
00:10:38.665 --> 00:10:42.650
we were actually able to
improve the ROUGE metric.

238
00:10:42.650 --> 00:10:45.880
One other thing that we did
offline was we did this much

239
00:10:45.880 --> 00:10:49.685
longer with a much
larger test dataset.

240
00:10:49.685 --> 00:10:52.390
It wasn't just the 10
or the 15 examples,

241
00:10:52.390 --> 00:10:54.290
this actually was
the full dataset,

242
00:10:54.290 --> 00:10:55.400
and let's take a look.

243
00:10:55.400 --> 00:10:57.215
That's what this file is.

244
00:10:57.215 --> 00:10:59.140
The CSV file that came

245
00:10:59.140 --> 00:11:01.520
along in this data
directory with this lab.

246
00:11:01.520 --> 00:11:04.940
Here we see with a
much larger dataset,

247
00:11:04.940 --> 00:11:07.840
the scores are still
pretty similar,

248
00:11:07.840 --> 00:11:09.895
where we're getting
close to double,

249
00:11:09.895 --> 00:11:12.160
not quite double in some cases,

250
00:11:12.160 --> 00:11:14.865
but pretty significant
improvement

251
00:11:14.865 --> 00:11:16.780
upon the original Flan-T5.

252
00:11:16.780 --> 00:11:20.845
Here we see the percentage
improvements specifically.

253
00:11:20.845 --> 00:11:22.625
If we actually do
the calculation,

254
00:11:22.625 --> 00:11:25.480
we see rouge1 is 18% higher,

255
00:11:25.480 --> 00:11:28.025
rouge2 10%, rougeL 13,

256
00:11:28.025 --> 00:11:31.540
rougeLsum 13.7 as well.

257
00:11:31.540 --> 00:11:36.230
Now let's get into parameter
efficient fine-tuning.

258
00:11:36.230 --> 00:11:38.265
This is one of my
favorite topics.

259
00:11:38.265 --> 00:11:40.745
This makes such a
big difference,

260
00:11:40.745 --> 00:11:42.890
especially when
you're constrained by

261
00:11:42.890 --> 00:11:45.500
how much compute
resources that you have,

262
00:11:45.500 --> 00:11:51.785
you can lower the footprint
both memory, disk, GPU, CPU,

263
00:11:51.785 --> 00:11:55.535
all of the resources
can be reduced

264
00:11:55.535 --> 00:11:59.825
just by introducing PEFT into
your fine-tuning process.

265
00:11:59.825 --> 00:12:02.345
In the lessons you
learned about LoRA,

266
00:12:02.345 --> 00:12:04.700
you learned about the rank.

267
00:12:04.700 --> 00:12:06.380
Here we're going to
choose rank of 32,

268
00:12:06.380 --> 00:12:08.660
which is actually
relatively high.

269
00:12:08.660 --> 00:12:13.250
But we are just
starting with that.

270
00:12:13.250 --> 00:12:17.225
Here it's the SEQ_2_SEQ_LM,
this is FLAN-T5.

271
00:12:17.225 --> 00:12:20.690
With just a few extra
lines of code here to

272
00:12:20.690 --> 00:12:25.445
configure our LoRA fine-tuning.

273
00:12:25.445 --> 00:12:29.060
Then here we see we're
only going to train

274
00:12:29.060 --> 00:12:33.950
1.4 percent of the
trainable model parameters.

275
00:12:33.950 --> 00:12:37.040
In a lot of cases
you can fine-tune

276
00:12:37.040 --> 00:12:40.370
very large models
on a single GPU.

277
00:12:40.370 --> 00:12:42.455
Here's some more of those
training arguments.

278
00:12:42.455 --> 00:12:43.610
This is really back to

279
00:12:43.610 --> 00:12:45.410
the original hugging
face training

280
00:12:45.410 --> 00:12:46.880
and training arguments,

281
00:12:46.880 --> 00:12:50.645
except instead of using
just the regular model,

282
00:12:50.645 --> 00:12:52.850
we are actually using
the PEFT model.

283
00:12:52.850 --> 00:12:55.430
Here this is a convenience
function offered by

284
00:12:55.430 --> 00:12:58.250
the PEFT library and we
give it the original model,

285
00:12:58.250 --> 00:12:59.960
which is the FLAN-T5.

286
00:12:59.960 --> 00:13:01.850
We give it the LoRA
configuration which

287
00:13:01.850 --> 00:13:03.935
we defined above
with the Rank 32.

288
00:13:03.935 --> 00:13:07.760
We say get me a PEFT
version of that model.

289
00:13:07.760 --> 00:13:09.845
That's what comes
out as 1.4 percent.

290
00:13:09.845 --> 00:13:11.885
Now we do the
training arguments.

291
00:13:11.885 --> 00:13:13.910
Again, small number of steps,

292
00:13:13.910 --> 00:13:16.760
small number of epochs here.

293
00:13:16.760 --> 00:13:20.300
We do have a version that
was trained offline.

294
00:13:20.300 --> 00:13:21.920
That's a little bit
better than the one

295
00:13:21.920 --> 00:13:24.110
that is in this
lab specifically,

296
00:13:24.110 --> 00:13:26.180
and that's what we're going
to download here in a sec.

297
00:13:26.180 --> 00:13:27.890
Let's do that.

298
00:13:27.890 --> 00:13:29.840
Here's the other model that was

299
00:13:29.840 --> 00:13:31.940
stored in S3 cloud storage.

300
00:13:31.940 --> 00:13:36.275
Now we see this is
only 14 megabytes.

301
00:13:36.275 --> 00:13:40.910
These are called the PEFT
adapters or LoRA adopters.

302
00:13:40.910 --> 00:13:45.110
These get merged or combined
with the original LLM.

303
00:13:45.110 --> 00:13:47.060
When you go to actually
serve this model,

304
00:13:47.060 --> 00:13:48.395
which we will hear in a bit,

305
00:13:48.395 --> 00:13:51.230
you have to take the
original LLM and then merge

306
00:13:51.230 --> 00:13:54.530
in this LoRA PEFT adapter.

307
00:13:54.530 --> 00:13:57.770
These are much smaller and
you can reuse the same base

308
00:13:57.770 --> 00:14:02.315
LLM and swap in different
PEFT adapters when needed.

309
00:14:02.315 --> 00:14:07.805
Now that we have the PEFT
adapter copied down from S3,

310
00:14:07.805 --> 00:14:10.835
we're going to merge that
with the original LLM,

311
00:14:10.835 --> 00:14:12.710
which is FLAN-T5 and use

312
00:14:12.710 --> 00:14:15.050
that to actually
perform summarization.

313
00:14:15.050 --> 00:14:17.510
Now one thing to call
out that's not entirely

314
00:14:17.510 --> 00:14:20.030
obvious is that when we do this,

315
00:14:20.030 --> 00:14:23.630
I can actually set the
is_trainable flag to false.

316
00:14:23.630 --> 00:14:26.285
By setting the is_trainable
flag to false,

317
00:14:26.285 --> 00:14:28.610
we are telling PyTorch
that we're not

318
00:14:28.610 --> 00:14:31.430
interested in
training this model.

319
00:14:31.430 --> 00:14:33.290
All we're interested in doing is

320
00:14:33.290 --> 00:14:36.290
the forward pass just
to get the summaries.

321
00:14:36.290 --> 00:14:39.020
This is significant
because we can tell

322
00:14:39.020 --> 00:14:42.440
PyTorch to not load
any of the update

323
00:14:42.440 --> 00:14:47.120
portions of these
operators and to

324
00:14:47.120 --> 00:14:49.625
basically minimize the footprint

325
00:14:49.625 --> 00:14:52.850
needed to just perform the
inference with this model.

326
00:14:52.850 --> 00:14:56.165
This is a pretty neat flag.

327
00:14:56.165 --> 00:14:59.285
This was actually just
introduced recently

328
00:14:59.285 --> 00:15:02.870
into the PEFT model at
the time of this lab.

329
00:15:02.870 --> 00:15:04.610
I wanted to show it here

330
00:15:04.610 --> 00:15:06.560
because this is a
pattern that you

331
00:15:06.560 --> 00:15:10.115
want to try to find when you're
doing your own modeling.

332
00:15:10.115 --> 00:15:11.690
When you know that
you're ready to

333
00:15:11.690 --> 00:15:13.655
deploy the model for inference,

334
00:15:13.655 --> 00:15:15.875
there are usually ways

335
00:15:15.875 --> 00:15:17.930
that you can hint
to the framework,

336
00:15:17.930 --> 00:15:20.795
such as PyTorch that you're
not going to be training.

337
00:15:20.795 --> 00:15:22.775
This can then further reduce

338
00:15:22.775 --> 00:15:25.385
the resources needed to
make these predictions.

339
00:15:25.385 --> 00:15:28.490
Here, just to emphasize it,

340
00:15:28.490 --> 00:15:31.175
I do print out the number
of trainable parameters.

341
00:15:31.175 --> 00:15:32.855
Keep in mind at
this point we are

342
00:15:32.855 --> 00:15:34.865
only planning to do inference,

343
00:15:34.865 --> 00:15:36.815
and let's move on to that.

344
00:15:36.815 --> 00:15:39.500
Zero percent of these
trainable parameters.

345
00:15:39.500 --> 00:15:41.540
Here, we're going to
build some sample prompts

346
00:15:41.540 --> 00:15:42.815
from our test data set.

347
00:15:42.815 --> 00:15:45.050
We're just going
to pick something

348
00:15:45.050 --> 00:15:48.005
randomly here,
essentially Index 200.

349
00:15:48.005 --> 00:15:51.590
We're going to see the
instruction model. Got it.

350
00:15:51.590 --> 00:15:52.955
Mostly right I think,

351
00:15:52.955 --> 00:15:56.105
the PEFT model
gets a little bit,

352
00:15:56.105 --> 00:15:59.180
starts to find a little
bit more nuance here.

353
00:15:59.180 --> 00:16:01.040
But really, as we'll see

354
00:16:01.040 --> 00:16:04.355
qualitatively when we
run the rouge metrics.

355
00:16:04.355 --> 00:16:06.470
Here we're going to compare
the human baseline to

356
00:16:06.470 --> 00:16:10.565
the original FLAN-T5 to the
instruction full fine-tuned,

357
00:16:10.565 --> 00:16:13.245
and then to the PEFT fine-tuned.

358
00:16:13.245 --> 00:16:15.370
For the most part,
just glancing here,

359
00:16:15.370 --> 00:16:18.130
it looks like these
are pretty similar.

360
00:16:18.130 --> 00:16:20.020
But let's take a look
at the rouge metrics

361
00:16:20.020 --> 00:16:21.190
and see what's going on.

362
00:16:21.190 --> 00:16:24.145
Here we see the
instruction fine-tuned was

363
00:16:24.145 --> 00:16:27.910
a pretty drastic improvement
over the original FLAN-T5.

364
00:16:27.910 --> 00:16:31.120
We see that the PEFT model
does suffer a little bit

365
00:16:31.120 --> 00:16:34.660
of a degradation from
the full fine-tuned.

366
00:16:34.660 --> 00:16:37.690
It's pretty close in some cases.

367
00:16:37.690 --> 00:16:39.295
It's not too bad.

368
00:16:39.295 --> 00:16:44.230
But we use much less
resources during fine-tuning,

369
00:16:44.230 --> 00:16:47.035
than we would have if we
did the full instruction.

370
00:16:47.035 --> 00:16:51.285
You can imagine this is only
just a few thousand samples,

371
00:16:51.285 --> 00:16:56.420
but you can imagine at scale
how this really can save you

372
00:16:56.420 --> 00:16:59.780
tons of compute
resources and time by

373
00:16:59.780 --> 00:17:03.035
using PEFT by looking
at the larger data set.

374
00:17:03.035 --> 00:17:06.275
Up above I was just looking
at maybe 10, 15 examples.

375
00:17:06.275 --> 00:17:08.120
Here we see larger.

376
00:17:08.120 --> 00:17:11.615
Looks like I think I
have it here rouge one,

377
00:17:11.615 --> 00:17:14.600
PEFT loses about one to maybe

378
00:17:14.600 --> 00:17:18.515
1.7 percent across all for
of these rouge metrics.

379
00:17:18.515 --> 00:17:21.410
That's not bad relative to

380
00:17:21.410 --> 00:17:26.940
the savings that you
get when you use PEFT.