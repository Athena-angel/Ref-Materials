Let's recap what
you've seen so far. Arlo HF is a fine-tuning process that aligns LLMs with
human preferences. In this process, you make use of a reward model to
assess and LLMs completions of a prompt data set against some human
preference metric, like helpful or not helpful. Next, you use a reinforcement
learning algorithm, in this case, PPO, to update the weights off the
LLM based on the reward is signed to the
completions generated by the current
version off the LLM. You'll carry out this cycle of a multiple iterations using
many different prompts and updates off
the model weights until you obtain your
desired degree of alignment. Your end result is a human aligned LLM that you
can use in your application. An interesting problem
that can emerge in reinforcement learning is
known as reward hacking, where the agent learns to cheat the system by favoring
actions that maximize the reward received even if those actions don't align well with the
original objective. In the context of LLMs, reward hacking can manifest as the addition of
words or phrases to completions that result in high scores for the
metric being aligned. But that reduce the overall
quality of the language. For example, suppose
you are using RHF to detoxify and
instruct model. You have already trained a reward model
that can carry out sentiment analysis and classify model completions as
toxic or non-toxic. You select a prompt from the training data
this product is, and pass it to the instruct an LLM which generates
a completion. This one, complete
garbage is not very nice and you can expect it
to get a high toxic rating. The completion is processed by the toxicity of reward model, which generates a score and this is fed to the PPO algorithm, which uses it to update
the model weights. As you iterate RHF will update the LLM to create a
less toxic responses. However, as the policy tries
to optimize the reward, it can diverge too much from
the initial language model. In this example, the model has started generating
completions that it has learned will lead to very low toxicity
scores by including phrases like most
awesome, most incredible. This language sounds
very exaggerated. The model could also start
generating nonsensical, grammatically incorrect
text that just happens to maximize the
rewards in a similar way, outputs like this are
definitely not very useful. To prevent our board
hacking from happening, you can use the initial instruct LLM as performance reference. Let's call it the
reference model. The weights of the reference
model are frozen and are not updated during
iterations of RHF. This way, you always maintain a single reference
model to compare to. During training, each prompt
is passed to both models, generating a completion
by the reference LLM and the intermediate
LLM updated model. At this point, you can
compare the two completions and calculate a value called the Kullback-Leibler
divergence, or KL divergence for short. KL divergence is a
statistical measure of how different two
probability distributions are. You can use it to compare the completions off
the two models and determine how much
the updated model has diverged from the reference. Don't worry too much about the
details of how this works. The KL divergence algorithm is included in many
standard machine learning libraries
and you can use it without knowing all
the math behind it. You'll actually make
use of KL divergence in this week's lab so you can see how this works for yourself. KL divergence is calculated
for each generate a token across the whole
vocabulary off the LLM. This can easily be
tens or hundreds of thousands of tokens. However, using a
softmax function, you've reduced the number
of probabilities to much less than the
full vocabulary size. Keep in mind that this is still a relatively compute
expensive process. You will almost always
benefit from using GPUs. Once you've calculated
the KL divergence between the two models, you added acid term to
the reward calculation. This will penalize the RL
updated model if it shifts too far from the reference LLM and generates completions
that are two different. Note that you now
need to full copies of the LLM to calculate
the KL divergence, the frozen reference LLM, and the oral updated PPO LLM. By the way, you can benefit from combining our
relationship with puffed. In this case, you only update the weights
of a path adapter, not the full weights of the LLM. This means that you can reuse the same underlying LLM for both the reference model
and the PPO model, which you update with a
trained path parameters. This reduces the
memory footprint during training by
approximately half. I know that there is a
lot to take in here, but don't worry, RHF with path is going to
be covered in the lab. If you'll get a
chance to see this in action and try it
out for yourself. Once you have completed your
RHF alignment of the model, you will want to assess
the model's performance. You can use the
summarization data set to quantify the
reduction in toxicity, for example, the dialogue, some data set that you saw
earlier in the course. The number you'll use here
is the toxicity score, this is the probability of the negative class,
in this case, a toxic or hateful response averaged across the completions. If RHF has successfully reduce
the toxicity of your LLM, this score should go down. First, you'll create a
baseline toxicity score for the original instruct
LLM by evaluating its completions off the
summarization data set with a reward model that
can assess toxic language. Then you'll evaluate your
newly human aligned model on the same data set
and compare the scores. In this example, the toxicity score has indeed
decreased after Arlo HF, indicating a less toxic,
better aligned model. Again, you'll see all of
this in this week's lab.