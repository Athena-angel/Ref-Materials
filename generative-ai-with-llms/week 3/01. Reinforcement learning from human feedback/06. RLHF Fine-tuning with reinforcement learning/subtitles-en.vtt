WEBVTT

1
00:00:00.000 --> 00:00:02.940
Let's bring everything together,

2
00:00:02.940 --> 00:00:05.820
and look at how you will
use the reward model in

3
00:00:05.820 --> 00:00:07.590
the reinforcement
learning process

4
00:00:07.590 --> 00:00:09.750
to update the LLM weights,

5
00:00:09.750 --> 00:00:11.640
and produce a human
aligned model.

6
00:00:11.640 --> 00:00:14.235
Remember, you want to
start with a model that

7
00:00:14.235 --> 00:00:17.715
already has good performance
on your task of interests.

8
00:00:17.715 --> 00:00:21.810
You'll work to align an
instruction finds you and LLM.

9
00:00:21.810 --> 00:00:25.770
First, you'll pass a prompt
from your prompt dataset.

10
00:00:25.770 --> 00:00:27.975
In this case, a dog is,

11
00:00:27.975 --> 00:00:29.280
to the instruct LLM,

12
00:00:29.280 --> 00:00:31.035
which then generates
a completion,

13
00:00:31.035 --> 00:00:33.525
in this case a furry animal.

14
00:00:33.525 --> 00:00:36.150
Next, you sent this completion,

15
00:00:36.150 --> 00:00:37.650
and the original prompt to

16
00:00:37.650 --> 00:00:40.815
the reward model as the
prompt completion pair.

17
00:00:40.815 --> 00:00:43.280
The reward model
evaluates the pair

18
00:00:43.280 --> 00:00:45.815
based on the human feedback
it was trained on,

19
00:00:45.815 --> 00:00:48.385
and returns a reward value.

20
00:00:48.385 --> 00:00:51.720
A higher value such
as 0.24 as shown

21
00:00:51.720 --> 00:00:55.025
here represents a more
aligned response.

22
00:00:55.025 --> 00:00:58.460
A less aligned response
would receive a lower value,

23
00:00:58.460 --> 00:01:00.980
such as negative 0.53.

24
00:01:00.980 --> 00:01:03.290
You'll then pass
this reward value

25
00:01:03.290 --> 00:01:05.090
for the prom completion pair to

26
00:01:05.090 --> 00:01:07.190
the reinforcement
learning algorithm to

27
00:01:07.190 --> 00:01:09.500
update the weights of the LLM,

28
00:01:09.500 --> 00:01:11.510
and move it towards
generating more

29
00:01:11.510 --> 00:01:14.150
aligned, higher
reward responses.

30
00:01:14.150 --> 00:01:16.580
Let's call this
intermediate version of

31
00:01:16.580 --> 00:01:19.505
the model the RL updated LLM.

32
00:01:19.505 --> 00:01:22.415
These series of steps together

33
00:01:22.415 --> 00:01:26.240
forms a single iteration
of the RLHF process.

34
00:01:26.240 --> 00:01:29.930
These iterations continue
for a given number of epics,

35
00:01:29.930 --> 00:01:32.755
similar to other
types of fine tuning.

36
00:01:32.755 --> 00:01:35.990
Here you can see that the
completion generated by

37
00:01:35.990 --> 00:01:39.695
the RL updated LLM receives
a higher reward score,

38
00:01:39.695 --> 00:01:42.125
indicating that the
updates to weights

39
00:01:42.125 --> 00:01:44.905
have resulted in a more
aligned completion.

40
00:01:44.905 --> 00:01:46.745
If the process is working well,

41
00:01:46.745 --> 00:01:48.575
you'll see the reward improving

42
00:01:48.575 --> 00:01:50.390
after each iteration
as the model

43
00:01:50.390 --> 00:01:52.010
produces text that is

44
00:01:52.010 --> 00:01:55.100
increasingly aligned
with human preferences.

45
00:01:55.100 --> 00:01:58.430
You will continue this iterative
process until your model

46
00:01:58.430 --> 00:02:01.320
is aligned based on some
evaluation criteria.

47
00:02:01.320 --> 00:02:03.935
For example, reaching
a threshold value

48
00:02:03.935 --> 00:02:06.025
for the helpfulness you defined.

49
00:02:06.025 --> 00:02:09.350
You can also define a
maximum number of steps,

50
00:02:09.350 --> 00:02:13.145
for example, 20,000 as
the stopping criteria.

51
00:02:13.145 --> 00:02:15.290
At this point, let's refer to

52
00:02:15.290 --> 00:02:18.710
the fine-tuned model as
the human-aligned LLM.

53
00:02:18.710 --> 00:02:21.410
One detail we haven't
discussed yet is

54
00:02:21.410 --> 00:02:25.055
the exact nature of the
reinforcement learning algorithm.

55
00:02:25.055 --> 00:02:27.440
This is the algorithm
that takes the output

56
00:02:27.440 --> 00:02:29.600
of the reward model and uses it

57
00:02:29.600 --> 00:02:31.760
to update the LLM
model weights so

58
00:02:31.760 --> 00:02:34.640
that the reward score
increases over time.

59
00:02:34.640 --> 00:02:37.550
There are several different
algorithms that you can

60
00:02:37.550 --> 00:02:40.445
use for this part of
the RLHF process.

61
00:02:40.445 --> 00:02:43.010
A popular choice is proximal

62
00:02:43.010 --> 00:02:46.480
policy optimization
or PPO for short.

63
00:02:46.480 --> 00:02:49.510
PPO is a pretty
complicated algorithm,

64
00:02:49.510 --> 00:02:51.440
and you don't have to
be familiar with all

65
00:02:51.440 --> 00:02:53.750
of the details to be
able to make use of it.

66
00:02:53.750 --> 00:02:55.460
However, it can be

67
00:02:55.460 --> 00:02:58.250
a tricky algorithm to
implement and understanding

68
00:02:58.250 --> 00:03:00.650
its inner workings in
more detail can help you

69
00:03:00.650 --> 00:03:02.180
troubleshoot if you're having

70
00:03:02.180 --> 00:03:03.770
problems getting it to work.

71
00:03:03.770 --> 00:03:07.070
To explain how the PPO
algorithm works in more detail,

72
00:03:07.070 --> 00:03:09.215
I invited my AWS colleague,

73
00:03:09.215 --> 00:03:13.315
Ek to give you a deeper dive
on the technical details.

74
00:03:13.315 --> 00:03:15.320
This next video is optional

75
00:03:15.320 --> 00:03:16.850
and you should feel
free to skip it,

76
00:03:16.850 --> 00:03:19.115
and move on to the
reward hacking video.

77
00:03:19.115 --> 00:03:21.200
You won't need the
information here to

78
00:03:21.200 --> 00:03:23.750
complete the quizzes
or this week's lab.

79
00:03:23.750 --> 00:03:26.150
However, I encourage
you to check out

80
00:03:26.150 --> 00:03:28.235
the details as RLHF is becoming

81
00:03:28.235 --> 00:03:30.725
increasingly important
to ensure that

82
00:03:30.725 --> 00:03:35.220
LLMs behave in a safe and
aligned manner in deployment.