WEBVTT

1
00:00:00.294 --> 00:00:06.056
The first step in fine-tuning an LLM with
RLHF is to select a model to work with and

2
00:00:06.056 --> 00:00:09.890
use it to prepare a data set for
human feedback.

3
00:00:09.890 --> 00:00:14.201
The model you choose should have some
capability to carry out the task you

4
00:00:14.201 --> 00:00:19.009
are interested in, whether this is text
summarization, question answering or

5
00:00:19.009 --> 00:00:20.082
something else.

6
00:00:20.082 --> 00:00:24.094
In general, you may find it easier
to start with an instruct model that

7
00:00:24.094 --> 00:00:29.690
has already been fine tuned across many
tasks and has some general capabilities.

8
00:00:29.690 --> 00:00:34.368
You'll then use this LLM along with
a prompt data set to generate a number of

9
00:00:34.368 --> 00:00:36.826
different responses for each prompt.

10
00:00:36.826 --> 00:00:40.184
The prompt dataset is
comprised of multiple prompts,

11
00:00:40.184 --> 00:00:44.910
each of which gets processed by the LLM
to produce a set of completions.

12
00:00:44.910 --> 00:00:48.805
The next step is to collect
feedback from human labelers on

13
00:00:48.805 --> 00:00:51.436
the completions generated by the LLM.

14
00:00:51.436 --> 00:00:55.375
This is the human feedback portion
of reinforcement learning with human

15
00:00:55.375 --> 00:00:56.218
feedback.

16
00:00:56.218 --> 00:01:00.840
First, you must decide what criterion
you want the humans to assess

17
00:01:00.840 --> 00:01:02.309
the completions on.

18
00:01:02.309 --> 00:01:08.470
This could be any of the issues discussed
so far like helpfulness or toxicity.

19
00:01:08.470 --> 00:01:10.997
Once you've decided, you will then ask

20
00:01:10.997 --> 00:01:15.991
the labelers to assess each completion
in the data set based on that criterion.

21
00:01:15.991 --> 00:01:18.490
Let's take a look at an example.

22
00:01:18.490 --> 00:01:22.681
In this case, the prompt is,
my house is too hot.

23
00:01:22.681 --> 00:01:24.547
You pass this prompt to the LLM,

24
00:01:24.547 --> 00:01:28.270
which then generates three
different completions.

25
00:01:28.270 --> 00:01:32.670
The task for your labelers is
to rank the three completions in

26
00:01:32.670 --> 00:01:37.169
order of helpfulness from the most
helpful to least helpful.

27
00:01:37.169 --> 00:01:42.954
So here the labeler will probably decide
that completion two is the most helpful.

28
00:01:42.954 --> 00:01:46.645
It tells the user something that
can actually cool their house and

29
00:01:46.645 --> 00:01:48.266
ranks as completion first.

30
00:01:48.266 --> 00:01:51.706
Neither completion one or
three are very helpful, but

31
00:01:51.706 --> 00:01:55.145
maybe the labeler will decide
that three is the worst of

32
00:01:55.145 --> 00:02:00.310
the two because the model actively
disagrees with the input from the user.

33
00:02:00.310 --> 00:02:06.122
So the labeler ranks the top completion
second and the last completion third.

34
00:02:06.122 --> 00:02:09.724
This process then gets repeated for
many prompt completion sets,

35
00:02:09.724 --> 00:02:13.324
building up a data set that can be
used to train the reward model that

36
00:02:13.324 --> 00:02:16.668
will ultimately carry out this
work instead of the humans.

37
00:02:16.668 --> 00:02:21.074
The same prompt completion
sets are usually assigned to

38
00:02:21.074 --> 00:02:25.199
multiple human labelers to
establish consensus and

39
00:02:25.199 --> 00:02:29.243
minimize the impact of poor
labelers in the group.

40
00:02:29.243 --> 00:02:33.653
Like the third labeler here, whose
responses disagree with the others and

41
00:02:33.653 --> 00:02:37.083
may indicate that they
misunderstood the instructions,

42
00:02:37.083 --> 00:02:39.652
this is actually a very important point.

43
00:02:39.652 --> 00:02:44.650
The clarity of your instructions can
make a big difference on the quality of

44
00:02:44.650 --> 00:02:46.805
the human feedback you obtain.

45
00:02:46.805 --> 00:02:51.692
Labelers are often drawn from samples of
the population that represent diverse and

46
00:02:51.692 --> 00:02:52.820
global thinking.

47
00:02:53.830 --> 00:02:58.391
Here you can see an example set of
instructions written for human labelers.

48
00:02:58.391 --> 00:03:02.896
This would be presented to the labeler
to read before beginning the task and

49
00:03:02.896 --> 00:03:07.450
made available to refer back to
as they work through the dataset.

50
00:03:07.450 --> 00:03:11.710
The instructions start with the overall
task the labeler should carry out.

51
00:03:11.710 --> 00:03:15.310
In this case, to choose the best
completion for the prompt.

52
00:03:15.310 --> 00:03:20.169
The instructions continue with additional
details to guide the labeler on

53
00:03:20.169 --> 00:03:21.872
how to complete the task.

54
00:03:21.872 --> 00:03:25.214
In general, the more detailed
you make these instructions,

55
00:03:25.214 --> 00:03:28.944
the higher the likelihood that
the labelers will understand the task

56
00:03:28.944 --> 00:03:32.850
they have to carry out and
complete it exactly as you wish.

57
00:03:32.850 --> 00:03:37.592
For instance, in the second instruction
item, the labelers are told that they

58
00:03:37.592 --> 00:03:41.841
should make decisions based on their
perception of the correctness and

59
00:03:41.841 --> 00:03:43.989
informativeness of the response.

60
00:03:43.989 --> 00:03:48.680
They are told they can use the Internet
to fact check and find other information.

61
00:03:49.690 --> 00:03:54.604
They are also given clear instructions
about what to do if they identify a tie,

62
00:03:54.604 --> 00:03:59.906
meaning a pair of completions that they
think are equally correct and informative.

63
00:03:59.906 --> 00:04:04.760
The labelers are told that it is okay
to rank two completions the same, but

64
00:04:04.760 --> 00:04:06.890
they should do this sparingly.

65
00:04:08.110 --> 00:04:12.868
A final instruction worth calling out
here is what to do in the case of

66
00:04:12.868 --> 00:04:16.302
a nonsensical confusing or
irrelevant answer.

67
00:04:16.302 --> 00:04:20.497
In this case, labelers should
select F rather than rank, so

68
00:04:20.497 --> 00:04:24.790
the poor quality answers
can be easily removed.

69
00:04:24.790 --> 00:04:28.851
Providing a detailed set of
instructions like this increases

70
00:04:28.851 --> 00:04:32.676
the likelihood that the responses
will be high quality and

71
00:04:32.676 --> 00:04:37.771
that individual humans will carry out
the task in a similar way to each other.

72
00:04:37.771 --> 00:04:42.568
This can help ensure that the ensemble
of labeled completions will be

73
00:04:42.568 --> 00:04:45.800
representative of
a consensus point of view.

74
00:04:46.810 --> 00:04:51.274
Once your human labelers have completed
their assessments off the Prom completion

75
00:04:51.274 --> 00:04:54.665
sets, you have all the data you
need to train the reward model.

76
00:04:54.665 --> 00:04:58.801
Which you will use instead of humans
to classify model completions during

77
00:04:58.801 --> 00:05:02.450
the reinforcement learning
finetuning process.

78
00:05:02.450 --> 00:05:05.552
Before you start to train
the reward model, however,

79
00:05:05.552 --> 00:05:10.149
you need to convert the ranking data into
a pairwise comparison of completions.

80
00:05:10.149 --> 00:05:15.124
In other words, all possible pairs of
completions from the available choices to

81
00:05:15.124 --> 00:05:18.206
a prompt should be classified as 0 or
1 score.

82
00:05:18.206 --> 00:05:22.749
In the example shown here,
there are three completions to a prompt,

83
00:05:22.749 --> 00:05:27.680
and the ranking assigned by the human
labelers was 2, 1, 3, as shown,

84
00:05:27.680 --> 00:05:32.724
where1 is the highest rank corresponding
to the most preferred response.

85
00:05:32.724 --> 00:05:37.284
With the three different completions,
there are three possible pairs

86
00:05:37.284 --> 00:05:41.630
purple-yellow, purple-green and
yellow-green.

87
00:05:41.630 --> 00:05:46.603
Depending on the number N of
alternative completions per prompt,

88
00:05:46.603 --> 00:05:49.723
you will have N choose two combinations.

89
00:05:49.723 --> 00:05:54.602
For each pair, you will assign a reward
of 1 for the preferred response and

90
00:05:54.602 --> 00:05:58.370
a reward of 0 for
the less preferred response.

91
00:05:58.370 --> 00:06:03.407
Then you'll reorder the prompts so
that the preferred option comes first.

92
00:06:03.407 --> 00:06:07.454
This is an important step
because the reward model expects

93
00:06:07.454 --> 00:06:12.370
the preferred completion,
which is referred to as Yj first.

94
00:06:12.370 --> 00:06:15.096
Once you have completed this data,
restructuring,

95
00:06:15.096 --> 00:06:19.227
the human responses will be in the correct
format for training the reward model.

96
00:06:19.227 --> 00:06:20.985
Note that while thumbs-up,

97
00:06:20.985 --> 00:06:25.768
thumbs-down feedback is often easier
to gather than ranking feedback, ranked

98
00:06:25.768 --> 00:06:30.149
feedback gives you more prom completion
data to train your reward model.

99
00:06:30.149 --> 00:06:34.550
As you can see, here you get three prompt
completion pairs from each human ranking.