The first step in fine-tuning an LLM with
RLHF is to select a model to work with and use it to prepare a data set for
human feedback. The model you choose should have some
capability to carry out the task you are interested in, whether this is text
summarization, question answering or something else. In general, you may find it easier
to start with an instruct model that has already been fine tuned across many
tasks and has some general capabilities. You'll then use this LLM along with
a prompt data set to generate a number of different responses for each prompt. The prompt dataset is
comprised of multiple prompts, each of which gets processed by the LLM
to produce a set of completions. The next step is to collect
feedback from human labelers on the completions generated by the LLM. This is the human feedback portion
of reinforcement learning with human feedback. First, you must decide what criterion
you want the humans to assess the completions on. This could be any of the issues discussed
so far like helpfulness or toxicity. Once you've decided, you will then ask the labelers to assess each completion
in the data set based on that criterion. Let's take a look at an example. In this case, the prompt is,
my house is too hot. You pass this prompt to the LLM, which then generates three
different completions. The task for your labelers is
to rank the three completions in order of helpfulness from the most
helpful to least helpful. So here the labeler will probably decide
that completion two is the most helpful. It tells the user something that
can actually cool their house and ranks as completion first. Neither completion one or
three are very helpful, but maybe the labeler will decide
that three is the worst of the two because the model actively
disagrees with the input from the user. So the labeler ranks the top completion
second and the last completion third. This process then gets repeated for
many prompt completion sets, building up a data set that can be
used to train the reward model that will ultimately carry out this
work instead of the humans. The same prompt completion
sets are usually assigned to multiple human labelers to
establish consensus and minimize the impact of poor
labelers in the group. Like the third labeler here, whose
responses disagree with the others and may indicate that they
misunderstood the instructions, this is actually a very important point. The clarity of your instructions can
make a big difference on the quality of the human feedback you obtain. Labelers are often drawn from samples of
the population that represent diverse and global thinking. Here you can see an example set of
instructions written for human labelers. This would be presented to the labeler
to read before beginning the task and made available to refer back to
as they work through the dataset. The instructions start with the overall
task the labeler should carry out. In this case, to choose the best
completion for the prompt. The instructions continue with additional
details to guide the labeler on how to complete the task. In general, the more detailed
you make these instructions, the higher the likelihood that
the labelers will understand the task they have to carry out and
complete it exactly as you wish. For instance, in the second instruction
item, the labelers are told that they should make decisions based on their
perception of the correctness and informativeness of the response. They are told they can use the Internet
to fact check and find other information. They are also given clear instructions
about what to do if they identify a tie, meaning a pair of completions that they
think are equally correct and informative. The labelers are told that it is okay
to rank two completions the same, but they should do this sparingly. A final instruction worth calling out
here is what to do in the case of a nonsensical confusing or
irrelevant answer. In this case, labelers should
select F rather than rank, so the poor quality answers
can be easily removed. Providing a detailed set of
instructions like this increases the likelihood that the responses
will be high quality and that individual humans will carry out
the task in a similar way to each other. This can help ensure that the ensemble
of labeled completions will be representative of
a consensus point of view. Once your human labelers have completed
their assessments off the Prom completion sets, you have all the data you
need to train the reward model. Which you will use instead of humans
to classify model completions during the reinforcement learning
finetuning process. Before you start to train
the reward model, however, you need to convert the ranking data into
a pairwise comparison of completions. In other words, all possible pairs of
completions from the available choices to a prompt should be classified as 0 or
1 score. In the example shown here,
there are three completions to a prompt, and the ranking assigned by the human
labelers was 2, 1, 3, as shown, where1 is the highest rank corresponding
to the most preferred response. With the three different completions,
there are three possible pairs purple-yellow, purple-green and
yellow-green. Depending on the number N of
alternative completions per prompt, you will have N choose two combinations. For each pair, you will assign a reward
of 1 for the preferred response and a reward of 0 for
the less preferred response. Then you'll reorder the prompts so
that the preferred option comes first. This is an important step
because the reward model expects the preferred completion,
which is referred to as Yj first. Once you have completed this data,
restructuring, the human responses will be in the correct
format for training the reward model. Note that while thumbs-up, thumbs-down feedback is often easier
to gather than ranking feedback, ranked feedback gives you more prom completion
data to train your reward model. As you can see, here you get three prompt
completion pairs from each human ranking.