WEBVTT

1
00:00:00.490 --> 00:00:04.970
This lesson on RLHF introduced
a lot of new terms and concepts.

2
00:00:04.970 --> 00:00:07.729
My colleague Chris has put
together an exciting lab

3
00:00:07.729 --> 00:00:11.010
that implements what you
explored in these videos.

4
00:00:11.010 --> 00:00:13.031
So to get your hands dirty with RLHF,

5
00:00:13.031 --> 00:00:17.050
I'll hand it over to Chris to walk
you through this week's notebook.

6
00:00:17.050 --> 00:00:20.948
>> Hey, thanks, Ancha, and
now it's time for Lab 3.

7
00:00:20.948 --> 00:00:26.410
All right, welcome back, everyone,
we are going to talk about Lab 3.

8
00:00:26.410 --> 00:00:29.676
Lab 3 was one of my favorite to work on.

9
00:00:29.676 --> 00:00:33.555
This is where we get hands on
with reinforcement learning and

10
00:00:33.555 --> 00:00:35.442
human feedback, or RLHF.

11
00:00:35.442 --> 00:00:40.299
The purpose of this lab is to lower
the toxicity of our instruction

12
00:00:40.299 --> 00:00:45.008
fine-tuned model from Lab 2,
that was the output of Lab 2.

13
00:00:45.008 --> 00:00:47.072
That'll be the input to Lab 3 here.

14
00:00:47.072 --> 00:00:51.655
And we will lower the toxicity
using a hate speech reward

15
00:00:51.655 --> 00:00:55.402
model where we want to optimize for
not hate.

16
00:00:55.402 --> 00:01:00.749
We'll be using PPO that you learned about
in the lesson, and we will wrap everything

17
00:01:00.749 --> 00:01:06.710
up with quantitative and then qualitative
comparison of our detoxification process.

18
00:01:06.710 --> 00:01:09.518
Okay, let's install our Python libraries.

19
00:01:09.518 --> 00:01:11.918
Here we see that we're using PyTorch.

20
00:01:11.918 --> 00:01:15.931
We're also using the same transformers
that we used in the previous labs,

21
00:01:15.931 --> 00:01:19.050
of course, data sets to get
access to our public data set,

22
00:01:19.050 --> 00:01:21.679
evaluate so
that we could run the rouge_score.

23
00:01:21.679 --> 00:01:25.010
Peft is the parameter
efficient fine-tuning library.

24
00:01:25.010 --> 00:01:27.196
And here's a new library called trl.

25
00:01:27.196 --> 00:01:30.282
And this is what's going
to give us access to PPO.

26
00:01:30.282 --> 00:01:34.258
And a similar type of trainer and
then training arguments,

27
00:01:34.258 --> 00:01:38.873
there's going to be PPOTrainer and
then training arguments following

28
00:01:38.873 --> 00:01:43.267
the hugging face convention of trainer and
training arguments.

29
00:01:43.267 --> 00:01:45.427
Let's do a bunch of our imports up here.

30
00:01:45.427 --> 00:01:49.721
Familiar classes, the AutoModelForSeq2Seq.

31
00:01:49.721 --> 00:01:53.818
Note that there's a new one here,
which is AutoModelForSeq1Classification.

32
00:01:53.818 --> 00:01:58.259
This is what we're going to use to
load our Facebook binary classifier,

33
00:01:58.259 --> 00:01:59.548
seq1 classifier.

34
00:01:59.548 --> 00:02:03.502
Which when we give it a string of text or
a sequence of text,

35
00:02:03.502 --> 00:02:08.105
it'll tell us whether or
not that text contains hate speech or not,

36
00:02:08.105 --> 00:02:12.114
with a particular distribution
across not hate or hate.

37
00:02:12.114 --> 00:02:16.098
And of course, in this lab,
we'll be optimizing for not hate.

38
00:02:16.098 --> 00:02:20.448
Here we see our Peft and
our LoraConfig, which we saw in Lab 2.

39
00:02:20.448 --> 00:02:23.110
Here we see the specific PPOTrainer.

40
00:02:23.110 --> 00:02:25.045
There's also this other class,

41
00:02:25.045 --> 00:02:28.576
AutoModelForSeqtoSeq language
model with a value head.

42
00:02:28.576 --> 00:02:32.868
This is what's required when we do the PPO
training, which we'll see in a bit.

43
00:02:32.868 --> 00:02:34.714
Also, this LengthSampler.

44
00:02:34.714 --> 00:02:37.493
This will give us the ability to filter or

45
00:02:37.493 --> 00:02:42.646
actually sample from our text various
lengths, so we don't actually have

46
00:02:42.646 --> 00:02:47.806
to pull in the entire sequence,
we could pull in just different samples.

47
00:02:47.806 --> 00:02:53.119
And this comes in handy when we're trying
to either sample from our full data set,

48
00:02:53.119 --> 00:02:56.670
even though it's beyond
the 512 context window.

49
00:02:56.670 --> 00:02:59.208
We may want to sample up to 512.

50
00:02:59.208 --> 00:02:59.945
In some cases,

51
00:02:59.945 --> 00:03:03.420
you would actually just throw away
samples that are larger than 512.

52
00:03:03.420 --> 00:03:07.385
With LengthSampler for example,
we can just sample the first 512 or

53
00:03:07.385 --> 00:03:11.812
take samples within the sequence,
maybe not starting from the beginning, but

54
00:03:11.812 --> 00:03:14.080
maybe in the middle or towards the end.

55
00:03:14.080 --> 00:03:15.904
And so LengthSampler is kind of nice.

56
00:03:15.904 --> 00:03:18.102
Here we see PyTorch, the evaluate.

57
00:03:18.102 --> 00:03:21.073
If you haven't seen this before tqdm,
this is

58
00:03:21.073 --> 00:03:26.024
where typically when you see progress
bars showing up in Jupyter Notebooks or

59
00:03:26.024 --> 00:03:29.695
on your command line,
they come from this tqdm library.

60
00:03:29.695 --> 00:03:33.537
As before,
we're going to load our data set and

61
00:03:33.537 --> 00:03:38.626
then eventually our model here in a sec,
we have this build data

62
00:03:38.626 --> 00:03:43.623
set Python function that we've
defined that can do the length

63
00:03:43.623 --> 00:03:48.907
sampling using that LengthSampler
that was mentioned up above,

64
00:03:48.907 --> 00:03:54.946
can also convert our text into those
vectors, also called tokenizing.

65
00:03:54.946 --> 00:03:59.024
So here's where we start to see a little
bit more complexity in our code.

66
00:03:59.024 --> 00:04:01.728
The first couple of labs
we stayed pretty simple.

67
00:04:01.728 --> 00:04:07.271
Lab 3, we start to do a little
bit more Pythonic type stuff.

68
00:04:07.271 --> 00:04:10.435
We have nested functions here to tokenize.

69
00:04:10.435 --> 00:04:14.013
We're starting to combine things
into single functions, and

70
00:04:14.013 --> 00:04:16.655
we're going to just say
build the data set here.

71
00:04:16.655 --> 00:04:18.404
We're going to give it the model name.

72
00:04:18.404 --> 00:04:21.870
We need the model name to know which
tokenizer to use, because again,

73
00:04:21.870 --> 00:04:24.186
these models each have
their own tokenizer.

74
00:04:24.186 --> 00:04:28.067
If you try to mix and match tokenizers
between different model types,

75
00:04:28.067 --> 00:04:31.534
that's not good, and
you'll see some pretty bad results.

76
00:04:31.534 --> 00:04:35.896
One other feature of this build
data set function is that we're

77
00:04:35.896 --> 00:04:39.694
going to wrap our data set
into this instruction prompt.

78
00:04:39.694 --> 00:04:42.242
So this is something that we did
in the first and second lab.

79
00:04:42.242 --> 00:04:45.692
So we're kind of combining it all
into one single function here.

80
00:04:45.692 --> 00:04:50.076
Now this is the model that we
trained in the second lab.

81
00:04:50.076 --> 00:04:52.384
So I'm pulling it from cloud storage here.

82
00:04:52.384 --> 00:04:55.749
We pull it down and
it should be in a folder here called

83
00:04:55.749 --> 00:05:00.640
peft-dialog-summary-checkpoint, let's
crack it open just to see.

84
00:05:00.640 --> 00:05:02.198
Here's the adapter_model.bin.

85
00:05:02.198 --> 00:05:05.482
Again, that's the thing,
that's only 14 megabyte.

86
00:05:05.482 --> 00:05:10.204
And here's our handy print number of
trainable model parameters function that

87
00:05:10.204 --> 00:05:14.558
we'll use from time to time to get
fancy and see when we're using peft.

88
00:05:14.558 --> 00:05:17.550
Throughout this entire lab,
we will be using peft.

89
00:05:17.550 --> 00:05:20.964
So for the most part,
we'll be just training and

90
00:05:20.964 --> 00:05:25.064
fine-tuning a very small
percentage of our model size.

91
00:05:25.064 --> 00:05:27.452
So here once again, 1.4%.

92
00:05:27.452 --> 00:05:33.114
Now here's where we have to use this
variant of the AutoModelForSeq2SeqLM,

93
00:05:33.114 --> 00:05:35.132
which is with ValueHead.

94
00:05:35.132 --> 00:05:42.518
So this is related to how PPO
actually performs its training.

95
00:05:42.518 --> 00:05:46.914
And here, by the way, just to be clear,
we do say is_trainable equals True.

96
00:05:46.914 --> 00:05:50.916
And that's on purpose because we
are actually putting the model into

97
00:05:50.916 --> 00:05:52.310
fine-tuning model.

98
00:05:52.310 --> 00:05:55.106
And then later on when we
go to make predictions and

99
00:05:55.106 --> 00:05:58.074
to generate summaries,
we will set that to false.

100
00:05:58.074 --> 00:06:03.076
So if you notice this number is a little
bit higher than it was in lab 2.

101
00:06:03.076 --> 00:06:06.794
And it's higher by exactly 769 parameters.

102
00:06:06.794 --> 00:06:08.664
And so
let me explain what's happening there.

103
00:06:08.664 --> 00:06:12.984
768 of those parameters
are from the ValueHead.

104
00:06:12.984 --> 00:06:15.448
Actually, all 769 come from the ValueHead.

105
00:06:15.448 --> 00:06:21.618
It's 768, which is the dimension
of the ValueHead plus our bias.

106
00:06:21.618 --> 00:06:26.457
And bias is very important,
by the way, because it's always there,

107
00:06:26.457 --> 00:06:31.206
people just tend to not include
it in their casual conversations.

108
00:06:31.206 --> 00:06:33.936
Now, I skipped over this
create reference model.

109
00:06:33.936 --> 00:06:35.483
This actually comes from TRL.

110
00:06:35.483 --> 00:06:37.398
We had imported this up above.

111
00:06:37.398 --> 00:06:42.300
And what this is doing is,
as you learned about in the lesson,

112
00:06:42.300 --> 00:06:47.391
when we do RLHF, you can specify
a base model that's used with KL

113
00:06:47.391 --> 00:06:53.427
divergence to make sure that while
the reward function is being optimized,

114
00:06:53.427 --> 00:06:57.954
right, while we are optimizing
to maximize the reward,

115
00:06:57.954 --> 00:07:01.118
which in this case is not hate speech.

116
00:07:01.118 --> 00:07:04.469
We don't want to just
wildly hack that reward and

117
00:07:04.469 --> 00:07:10.402
just generate things that are not hate,
but that are not relevant to the original.

118
00:07:10.402 --> 00:07:12.930
So said differently, when we go to train,

119
00:07:12.930 --> 00:07:17.007
we're going to pass in two models,
what's called a reference model,

120
00:07:17.007 --> 00:07:21.750
which is a model that is not going to be
fine-tuned at all, not even with peft.

121
00:07:21.750 --> 00:07:25.104
It's just the original
instruction fine-tuned model

122
00:07:25.104 --> 00:07:28.960
that was the output of Lab 2,
that's now the input for Lab 3.

123
00:07:28.960 --> 00:07:33.385
And then KL divergence is used to
compare what would the original model

124
00:07:33.385 --> 00:07:38.185
have generated versus what would
the current PPO model have generated, and

125
00:07:38.185 --> 00:07:40.960
then keeps things sort
of in line that way and

126
00:07:40.960 --> 00:07:45.466
then minimizes the model's ability
to perform the reward hacking.

127
00:07:45.466 --> 00:07:47.240
So we'll see it here in a bit.

128
00:07:47.240 --> 00:07:51.400
Here's where we're actually going to load,
what I'm calling the toxicity model.

129
00:07:51.400 --> 00:07:52.863
Again, this comes from Facebook.

130
00:07:52.863 --> 00:07:57.724
It was designed to detect hate speech,
it's based on Bert.

131
00:07:57.724 --> 00:08:01.298
Facebook has a variant
of Bert called Roberta.

132
00:08:01.298 --> 00:08:06.547
So this model is on the order of millions
of parameters, not in the billions and

133
00:08:06.547 --> 00:08:10.784
billions like the large language
models that we have today.

134
00:08:10.784 --> 00:08:14.469
Now that that model is loaded, so again,
here's where we use the auto model for

135
00:08:14.469 --> 00:08:15.974
sequence classification.

136
00:08:15.974 --> 00:08:17.923
So this is a classifier.

137
00:08:17.923 --> 00:08:20.388
In this case, there's only two labels and

138
00:08:20.388 --> 00:08:25.329
that's what is going to make this a binary
classifier, it's not hate or it's hate.

139
00:08:25.329 --> 00:08:27.652
And we'll see that in action here.

140
00:08:27.652 --> 00:08:30.372
Here is a sample, nontoxic text.

141
00:08:30.372 --> 00:08:35.528
I want to kiss you,
pretty friendly piece of text here.

142
00:08:35.528 --> 00:08:40.289
And we see different ways that this
model can actually generate output for

143
00:08:40.289 --> 00:08:43.384
this text,
can actually classify this text.

144
00:08:43.384 --> 00:08:45.061
Just know that the first spot,

145
00:08:45.061 --> 00:08:49.008
the most important thing is the first
slot that this model can generate.

146
00:08:49.008 --> 00:08:53.564
The first class is not hate, and
that becomes very important.

147
00:08:53.564 --> 00:08:59.930
Hate is in the second position,
right or 0 index, it's the first index.

148
00:08:59.930 --> 00:09:04.246
So 0 index is not hate,
first index, the 1 index is hate.

149
00:09:04.246 --> 00:09:07.088
And we see that not hate is very high.

150
00:09:07.088 --> 00:09:09.200
So these are the logits, right?

151
00:09:09.200 --> 00:09:14.820
We can also, of course,
perform a soft max on those logits and

152
00:09:14.820 --> 00:09:20.008
we see that the probability of
this text being not toxic or

153
00:09:20.008 --> 00:09:23.588
not hate, is essentially 100%.

154
00:09:23.588 --> 00:09:30.276
The probability that this model is or
that this text is hate is very close to 0.

155
00:09:30.276 --> 00:09:33.492
In our PPO training,
we will actually use the logit, and

156
00:09:33.492 --> 00:09:36.124
it's very important to
get the index right.

157
00:09:36.124 --> 00:09:40.147
And I say this because if you
accidentally optimize for

158
00:09:40.147 --> 00:09:45.307
the wrong index, you can actually
generate text that's more toxic,

159
00:09:45.307 --> 00:09:48.910
which is not what we're
trying to do here today.

160
00:09:48.910 --> 00:09:53.880
And one thing that I did to actually
make this more explicit is I call out

161
00:09:53.880 --> 00:09:55.996
the not hate index equals 0.

162
00:09:55.996 --> 00:10:01.197
And the reason why this is confusing is
in a lot of cases, the sort of positive

163
00:10:01.197 --> 00:10:06.094
class is in the second position, and
the negative class is in the 0th.

164
00:10:06.094 --> 00:10:10.960
And so if you go and copy somebody else's
code or you find an example online, or

165
00:10:10.960 --> 00:10:15.487
you have a friend help you, you might
accidentally mess up which index.

166
00:10:15.487 --> 00:10:18.877
Here is an example of toxic text,
and now keep in mind,

167
00:10:18.877 --> 00:10:22.552
I did have to use a bad word
here to actually trigger this.

168
00:10:22.552 --> 00:10:24.824
And so let's run it there.

169
00:10:24.824 --> 00:10:28.095
And so here we see 97, almost 97 and

170
00:10:28.095 --> 00:10:33.378
a half percent probability that
this is toxic or hate speech.

171
00:10:33.378 --> 00:10:35.506
Now, hate speech is extreme.

172
00:10:35.506 --> 00:10:40.558
And so I do caution,
if you do try to play with this example,

173
00:10:40.558 --> 00:10:46.310
that oftentimes you do have to be
extreme to trigger the hate flag.

174
00:10:46.310 --> 00:10:50.227
So the good news, is we're not optimizing
for the hate flag, we're optimizing for

175
00:10:50.227 --> 00:10:52.292
the not hate flag, so let's keep going.

176
00:10:52.292 --> 00:10:55.838
One thing we haven't
shown much in these labs

177
00:10:55.838 --> 00:11:00.234
is what's called a hugging
face inference pipeline.

178
00:11:00.234 --> 00:11:03.130
And so this gets imported up above,
this was imported.

179
00:11:03.130 --> 00:11:06.314
And here we're creating what's
called a sentiment pipeline.

180
00:11:06.314 --> 00:11:09.438
And the real value of these
hugging face pipelines,

181
00:11:09.438 --> 00:11:11.902
this is from the transformers library.

182
00:11:11.902 --> 00:11:16.444
The real value is I can just say this
is a sentiment analysis problem,

183
00:11:16.444 --> 00:11:19.266
basically 0, or sorry, two classes.

184
00:11:19.266 --> 00:11:24.060
And I give it the name of my model and
then I can use this.

185
00:11:24.060 --> 00:11:28.359
And I don't have to call all those
low level model.generate and

186
00:11:28.359 --> 00:11:33.328
do the tokenizer and all that stuff,
this will actually do it all for us.

187
00:11:33.328 --> 00:11:37.161
So again, we did not use these
in the first and second lab,

188
00:11:37.161 --> 00:11:41.308
we are using it here just to reduce
the complexity of this lab and

189
00:11:41.308 --> 00:11:44.224
really, really focus on the RL part of it.

190
00:11:44.224 --> 00:11:47.891
But I do want to show you that there
are these things called the inference

191
00:11:47.891 --> 00:11:50.410
pipelines, and they're very, very handy.

192
00:11:50.410 --> 00:11:52.929
You can mix and
match as we're doing in this lab here.

193
00:11:52.929 --> 00:11:56.084
Sometimes you can use them,
sometimes you don't have to use them.

194
00:11:56.084 --> 00:12:02.023
Here we are setting up
a toxicity evaluation mechanism.

195
00:12:02.023 --> 00:12:06.813
And we're going to reuse
the Evaluate Python library which

196
00:12:06.813 --> 00:12:10.754
does have a first class
knowledge of toxicity.

197
00:12:10.754 --> 00:12:15.328
And so of course, when we set this up,
we have to give it the toxicity model,

198
00:12:15.328 --> 00:12:19.564
which in this case we're using
Facebook RoBERTa hate speech model.

199
00:12:19.564 --> 00:12:22.790
The other interesting thing is that
we have to pass in the toxic label.

200
00:12:22.790 --> 00:12:24.950
In this case, the toxic label is hate.

201
00:12:24.950 --> 00:12:29.923
And we will use this evaluator
later on when we compare before and

202
00:12:29.923 --> 00:12:35.578
then after our PPO, our reinforcement
learning with human feedback.

203
00:12:35.578 --> 00:12:40.727
Here we see for the nontoxic text which we
specified up above, I want to kiss you and

204
00:12:40.727 --> 00:12:46.318
the toxic text, we see that these scores
are consistent with what we are expecting.

205
00:12:46.318 --> 00:12:50.664
Here is a convenience function where we
pass in the model, pass in the data set.

206
00:12:50.664 --> 00:12:55.162
We can even tell it the number
of samples to try and

207
00:12:55.162 --> 00:13:00.539
to calculate a mean toxicity
score along with the standard

208
00:13:00.539 --> 00:13:04.709
deviation for
a whole bunch of summaries that

209
00:13:04.709 --> 00:13:09.846
are generated tell us what
is the mean toxicity score.

210
00:13:09.846 --> 00:13:16.110
And so the goal is to reduce the mean
toxicity score after we do PPO.

211
00:13:16.110 --> 00:13:21.814
And so here we see what the mean and
standard deviation toxicity is before

212
00:13:21.814 --> 00:13:28.554
we perform our detoxification to reduce
the toxicity of our generated summaries.

213
00:13:28.554 --> 00:13:31.370
Here we're going to
initialize the PPOTrainer.

214
00:13:31.370 --> 00:13:33.508
We have a PPOConfig as well too.

215
00:13:33.508 --> 00:13:36.782
These are really just sort
of standard learning rates.

216
00:13:36.782 --> 00:13:39.323
We are doing a small
number of the PPO epics.

217
00:13:39.323 --> 00:13:45.256
Again, just to keep the time for
this lab down to a reasonable number.

218
00:13:45.256 --> 00:13:48.108
Batch_size, here, 16, not too bad.

219
00:13:48.108 --> 00:13:53.452
And you can actually play with some of
these values when you get into the lab.

220
00:13:53.452 --> 00:13:56.444
Here we see PPOTrainer
takes the reference model.

221
00:13:56.444 --> 00:13:59.824
So this we used up above when we
called Create_Reference model.

222
00:13:59.824 --> 00:14:05.664
And that's essentially the original
model that was the output of the Lab 2.

223
00:14:05.664 --> 00:14:10.896
And that's what's going to be used for
KL divergence during PPO training.

224
00:14:10.896 --> 00:14:14.330
Now we actually fine-tune
the model using RLHF.

225
00:14:14.330 --> 00:14:15.956
This is the exciting part.

226
00:14:15.956 --> 00:14:21.229
And so here we see kl divergence,
which is something we want to minimize,

227
00:14:21.229 --> 00:14:23.878
just like you learned in the lesson.

228
00:14:23.878 --> 00:14:27.704
We don't want the KL divergence to go
too high, that means that the model is

229
00:14:27.704 --> 00:14:30.936
starting to hack the reward function,
and that's not good.

230
00:14:30.936 --> 00:14:33.952
It's just generating things
to make the reward happy,

231
00:14:33.952 --> 00:14:37.995
it's not generating things that
are actually similar to the original text

232
00:14:37.995 --> 00:14:40.610
that's being passed in for summarization.

233
00:14:40.610 --> 00:14:43.312
And we're trying to
maximize the mean return.

234
00:14:43.312 --> 00:14:47.712
These are some of the PPO reinforcement
learning concepts that you learned about

235
00:14:47.712 --> 00:14:48.559
in the slides.

236
00:14:48.559 --> 00:14:52.560
Maximize the advantage, and
this will run for about 20 minutes or so.

237
00:14:52.560 --> 00:14:54.390
So go grab a cup of coffee.

238
00:14:54.390 --> 00:14:57.468
And what's happening here is,

239
00:14:57.468 --> 00:15:02.484
we're grabbing each of
the samples that we provided

240
00:15:02.484 --> 00:15:08.080
from our dialog data set and
we're summarizing the text.

241
00:15:09.090 --> 00:15:14.697
We're using the sentiment pipeline that we
created up above that's going to classify

242
00:15:14.697 --> 00:15:19.944
both what's called the query, which is
the prompt and the response together.

243
00:15:19.944 --> 00:15:24.159
So we're going to zip those two
together using the python zip.

244
00:15:24.159 --> 00:15:28.347
And we have query response we're
going to pass that pair in to

245
00:15:28.347 --> 00:15:32.866
the sentiment pipeline and
ask the pipeline, is this toxic?

246
00:15:32.866 --> 00:15:35.058
Is this hate or is this not hate?

247
00:15:35.058 --> 00:15:41.014
That then gives us the logits for
those query response pairs.

248
00:15:41.014 --> 00:15:44.576
We're going to pull out
the not_hate score and

249
00:15:44.576 --> 00:15:48.662
we're going to pass all of
this in to the PPOTrainer.

250
00:15:48.662 --> 00:15:52.025
So we're going to give it prompt,
we're going to give it the response or

251
00:15:52.025 --> 00:15:54.538
the summary and
we're going to give it the score.

252
00:15:54.538 --> 00:15:59.706
All of those three go in as lists,
as batches into our PPOTrainer.

253
00:15:59.706 --> 00:16:04.553
That's then going to perform a PPO
step to minimize the loss function

254
00:16:04.553 --> 00:16:06.414
as you saw in the slides.

255
00:16:06.414 --> 00:16:09.646
And this is where the actual
gradients are being updated.

256
00:16:09.646 --> 00:16:12.238
And now keep in mind
that we're using peft.

257
00:16:12.238 --> 00:16:18.167
And so we're not actually modifying
the base LOM parameters peft,

258
00:16:18.167 --> 00:16:22.848
we are just modifying
the 1.4% lorapeftadapters

259
00:16:22.848 --> 00:16:27.900
that are being used during
this fine-tuning process.

260
00:16:27.900 --> 00:16:30.784
And we will run this for a bit.

261
00:16:30.784 --> 00:16:36.192
We see KL divergence is relatively stable,
around 27, 28, 29.

262
00:16:36.192 --> 00:16:40.730
It goes up, it comes down,
that's PPO trying to keep things balanced.

263
00:16:40.730 --> 00:16:42.459
And once this is done,

264
00:16:42.459 --> 00:16:47.755
we will compare the model quantitatively
as well as qualitatively.

265
00:16:49.570 --> 00:16:55.448
After about ten iterations, we can compare
the model that has been detoxified,

266
00:16:55.448 --> 00:16:59.864
or has reduced toxicity as
compared to the original model.

267
00:16:59.864 --> 00:17:04.429
And here we could do this qualitatively
by comparing the previous

268
00:17:04.429 --> 00:17:06.872
response to the response after.

269
00:17:06.872 --> 00:17:11.472
So here we see that the reward has
actually gone up in a number of

270
00:17:11.472 --> 00:17:13.458
cases of these examples.

271
00:17:13.458 --> 00:17:16.827
So query is the original text
that we want to summarize,

272
00:17:16.827 --> 00:17:20.933
the original conversation wrapped
in that instruction prompt and

273
00:17:20.933 --> 00:17:24.512
we see the response before,
we see the response after.

274
00:17:24.512 --> 00:17:28.619
And we see that the model has
determined that the reward model,

275
00:17:28.619 --> 00:17:31.332
the hate speech model has determined that

276
00:17:31.332 --> 00:17:35.300
the reward actually is more
positive in a number of these.

277
00:17:35.300 --> 00:17:38.556
If you were to train longer and
had more time,

278
00:17:38.556 --> 00:17:42.868
you would see this a relatively
significant difference.

279
00:17:42.868 --> 00:17:47.710
The other thing to mention is,
because the reward model is so extreme,

280
00:17:47.710 --> 00:17:52.473
that really to get the most benefit,
you would start with a relatively

281
00:17:52.473 --> 00:17:56.993
toxic data set, which this is not
a relatively toxic data set, and

282
00:17:56.993 --> 00:18:00.268
then you would see a much
larger difference.

283
00:18:00.268 --> 00:18:03.933
But here we see directionally
by doing PPO and

284
00:18:03.933 --> 00:18:09.141
using the reward function that
we're using the reward model for

285
00:18:09.141 --> 00:18:13.674
the hate speech,
that we can actually lower the overall

286
00:18:13.674 --> 00:18:17.940
toxicity of our model
responses by a decent amount.