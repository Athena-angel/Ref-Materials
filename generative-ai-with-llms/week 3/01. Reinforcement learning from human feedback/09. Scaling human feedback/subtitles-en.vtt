WEBVTT

1
00:00:00.000 --> 00:00:01.995
Although you can
use a reward model

2
00:00:01.995 --> 00:00:03.360
to eliminate the need for

3
00:00:03.360 --> 00:00:06.630
human evaluation during
RLHF fine tuning,

4
00:00:06.630 --> 00:00:08.700
the human effort
required to produce

5
00:00:08.700 --> 00:00:11.760
the trained reward model in
the first place is huge.

6
00:00:11.760 --> 00:00:13.770
The labeled data
set used to train

7
00:00:13.770 --> 00:00:15.180
the reward model typically

8
00:00:15.180 --> 00:00:17.355
requires large
teams of labelers,

9
00:00:17.355 --> 00:00:19.170
sometimes many thousands of

10
00:00:19.170 --> 00:00:22.410
people to evaluate
many prompts each.

11
00:00:22.410 --> 00:00:24.960
This work requires
a lot of time and

12
00:00:24.960 --> 00:00:29.085
other resources which can be
important limiting factors.

13
00:00:29.085 --> 00:00:32.310
As the number of models
and use cases increases,

14
00:00:32.310 --> 00:00:35.445
human effort becomes
a limited resource.

15
00:00:35.445 --> 00:00:37.905
Methods to scale human feedback

16
00:00:37.905 --> 00:00:40.420
are an active area of research.

17
00:00:40.420 --> 00:00:43.310
One idea to overcome
these limitations

18
00:00:43.310 --> 00:00:46.490
is to scale through
model self supervision.

19
00:00:46.490 --> 00:00:51.460
Constitutional AI is one
approach of scale supervision.

20
00:00:51.460 --> 00:00:56.150
First proposed in 2022 by
researchers at Anthropic,

21
00:00:56.150 --> 00:00:59.840
Constitutional AI is a method
for training models using

22
00:00:59.840 --> 00:01:01.430
a set of rules and

23
00:01:01.430 --> 00:01:04.325
principles that govern
the model's behavior.

24
00:01:04.325 --> 00:01:06.995
Together with a set
of sample prompts,

25
00:01:06.995 --> 00:01:09.350
these form the constitution.

26
00:01:09.350 --> 00:01:12.770
You then train the model to
self critique and revise

27
00:01:12.770 --> 00:01:16.075
its responses to comply
with those principles.

28
00:01:16.075 --> 00:01:18.260
Constitutional AI is useful

29
00:01:18.260 --> 00:01:20.030
not only for scaling feedback,

30
00:01:20.030 --> 00:01:21.530
it can also help address

31
00:01:21.530 --> 00:01:25.445
some unintended
consequences of RLHF.

32
00:01:25.445 --> 00:01:29.300
For example, depending on how
the prompt is structured,

33
00:01:29.300 --> 00:01:31.430
an aligned model may
end up revealing

34
00:01:31.430 --> 00:01:33.650
harmful information as it tries

35
00:01:33.650 --> 00:01:36.470
to provide the most
helpful response it can.

36
00:01:36.470 --> 00:01:39.680
As an example, imagine
you ask the model to

37
00:01:39.680 --> 00:01:40.970
give you instructions on

38
00:01:40.970 --> 00:01:43.085
how to hack your
neighbor's WiFi.

39
00:01:43.085 --> 00:01:45.320
Because this model
has been aligned

40
00:01:45.320 --> 00:01:47.330
to prioritize helpfulness,

41
00:01:47.330 --> 00:01:50.645
it actually tells you about
an app that lets you do this,

42
00:01:50.645 --> 00:01:53.645
even though this
activity is illegal.

43
00:01:53.645 --> 00:01:56.120
Providing the model with a set

44
00:01:56.120 --> 00:01:58.310
of constitutional principles can

45
00:01:58.310 --> 00:01:59.570
help the model balance

46
00:01:59.570 --> 00:02:03.235
these competing interests
and minimize the harm.

47
00:02:03.235 --> 00:02:06.830
Here are some example rules
from the research paper

48
00:02:06.830 --> 00:02:10.060
that Constitutional AI
I asks LLMs to follow.

49
00:02:10.060 --> 00:02:12.470
For example, you can
tell the model to choose

50
00:02:12.470 --> 00:02:13.940
the response that is the most

51
00:02:13.940 --> 00:02:16.120
helpful, honest, and harmless.

52
00:02:16.120 --> 00:02:18.050
But you can play
some bounds on this,

53
00:02:18.050 --> 00:02:20.870
asking the model to
prioritize harmlessness by

54
00:02:20.870 --> 00:02:23.765
assessing whether it's
response encourages illegal,

55
00:02:23.765 --> 00:02:26.180
unethical, or immoral activity.

56
00:02:26.180 --> 00:02:27.470
Note that you don't have to

57
00:02:27.470 --> 00:02:28.780
use the rules from the paper,

58
00:02:28.780 --> 00:02:31.190
you can define your own
set of rules that is best

59
00:02:31.190 --> 00:02:33.995
suited for your
domain and use case.

60
00:02:33.995 --> 00:02:37.250
When implementing the
Constitutional AI method,

61
00:02:37.250 --> 00:02:40.190
you train your model in
two distinct phases.

62
00:02:40.190 --> 00:02:43.610
In the first stage, you carry
out supervised learning,

63
00:02:43.610 --> 00:02:45.920
to start your prompt
the model in ways that

64
00:02:45.920 --> 00:02:48.955
try to get it to generate
harmful responses,

65
00:02:48.955 --> 00:02:51.350
this process is
called red teaming.

66
00:02:51.350 --> 00:02:53.690
You then ask the
model to critique

67
00:02:53.690 --> 00:02:55.970
its own harmful
responses according to

68
00:02:55.970 --> 00:02:58.130
the constitutional
principles and

69
00:02:58.130 --> 00:03:01.010
revise them to comply
with those rules.

70
00:03:01.010 --> 00:03:02.220
Once done, you'll fine-tune

71
00:03:02.220 --> 00:03:04.340
the model using the
pairs of red team

72
00:03:04.340 --> 00:03:08.225
prompts and the revised
constitutional responses.

73
00:03:08.225 --> 00:03:10.070
Let's look at an example of

74
00:03:10.070 --> 00:03:13.220
how one of these prompt
completion pairs is generated.

75
00:03:13.220 --> 00:03:16.160
Let's return to the
WiFi hacking problem.

76
00:03:16.160 --> 00:03:17.495
As you saw earlier,

77
00:03:17.495 --> 00:03:19.715
this model gives you
a harmful response

78
00:03:19.715 --> 00:03:22.370
as it tries to maximize
its helpfulness.

79
00:03:22.370 --> 00:03:24.695
To mitigate this, you
augment the prompt

80
00:03:24.695 --> 00:03:26.930
using the harmful completion and

81
00:03:26.930 --> 00:03:29.180
a set of predefined
instructions that

82
00:03:29.180 --> 00:03:31.825
ask the model to
critique its response.

83
00:03:31.825 --> 00:03:34.625
Using the rules outlined
in the Constitution,

84
00:03:34.625 --> 00:03:38.045
the model detects the
problems in its response.

85
00:03:38.045 --> 00:03:40.610
In this case, it
correctly acknowledges

86
00:03:40.610 --> 00:03:43.340
that hacking into
someone's WiFi is illegal.

87
00:03:43.340 --> 00:03:45.410
Lastly, you put all the parts

88
00:03:45.410 --> 00:03:47.420
together and ask
the model to write

89
00:03:47.420 --> 00:03:49.250
a new response that removes

90
00:03:49.250 --> 00:03:51.745
all of the harmful
or illegal content.

91
00:03:51.745 --> 00:03:54.275
The model generates a new answer

92
00:03:54.275 --> 00:03:56.435
that puts the
constitutional principles

93
00:03:56.435 --> 00:03:58.400
into practice and does not

94
00:03:58.400 --> 00:04:01.420
include the reference
to the illegal app.

95
00:04:01.420 --> 00:04:03.875
The original red team prompt,

96
00:04:03.875 --> 00:04:06.470
and this final
constitutional response

97
00:04:06.470 --> 00:04:08.540
can then be used
as training data.

98
00:04:08.540 --> 00:04:10.190
You'll build up a data set of

99
00:04:10.190 --> 00:04:11.930
many examples like
this to create

100
00:04:11.930 --> 00:04:14.300
a fine-tuned NLM
that has learned how

101
00:04:14.300 --> 00:04:17.785
to generate
constitutional responses.

102
00:04:17.785 --> 00:04:19.715
The second part of

103
00:04:19.715 --> 00:04:22.370
the process performs
reinforcement learning.

104
00:04:22.370 --> 00:04:24.890
This stage is similar to RLHF,

105
00:04:24.890 --> 00:04:27.305
except that instead
of human feedback,

106
00:04:27.305 --> 00:04:30.560
we now use feedback
generated by a model.

107
00:04:30.560 --> 00:04:34.070
This is sometimes referred
to as reinforcement learning

108
00:04:34.070 --> 00:04:37.795
from AI feedback or RLAIF.

109
00:04:37.795 --> 00:04:40.700
Here you use the
fine-tuned model from

110
00:04:40.700 --> 00:04:42.560
the previous step to generate

111
00:04:42.560 --> 00:04:45.070
a set of responses
to your prompt.

112
00:04:45.070 --> 00:04:48.290
You then ask the model
which of the responses is

113
00:04:48.290 --> 00:04:51.800
preferred according to the
constitutional principles.

114
00:04:51.800 --> 00:04:54.290
The result is a model generated

115
00:04:54.290 --> 00:04:55.790
preference dataset that you

116
00:04:55.790 --> 00:04:58.025
can use to train a reward model.

117
00:04:58.025 --> 00:04:59.810
With this reward model, you can

118
00:04:59.810 --> 00:05:01.730
now fine-tune your model further

119
00:05:01.730 --> 00:05:03.965
using a reinforcement
learning algorithm

120
00:05:03.965 --> 00:05:06.470
like PPO, as discussed earlier.

121
00:05:06.470 --> 00:05:09.710
Aligning models is a
very important topic

122
00:05:09.710 --> 00:05:12.125
and an active area of research.

123
00:05:12.125 --> 00:05:15.020
The foundations of RLHF
that you've explored in

124
00:05:15.020 --> 00:05:16.790
this lesson will allow you to

125
00:05:16.790 --> 00:05:19.405
follow along as
the field evolves.

126
00:05:19.405 --> 00:05:21.800
I'm really excited to see

127
00:05:21.800 --> 00:05:24.830
what new discoveries
researchers make in this area.

128
00:05:24.830 --> 00:05:28.055
I encourage you to keep an
eye out for any new methods

129
00:05:28.055 --> 00:05:29.240
and best practices that

130
00:05:29.240 --> 00:05:32.070
emerge in the coming
months and years