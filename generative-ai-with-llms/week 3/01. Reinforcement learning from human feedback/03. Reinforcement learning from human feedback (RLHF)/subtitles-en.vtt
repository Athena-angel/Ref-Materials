WEBVTT

1
00:00:00.330 --> 00:00:03.782
Let's consider the task
of text summarization,

2
00:00:03.782 --> 00:00:08.548
where you use the model to generate
a short piece of text that captures

3
00:00:08.548 --> 00:00:11.845
the most important points
in a longer article.

4
00:00:11.845 --> 00:00:17.138
Your goal is to use fine-tuning to
improve the model's ability to summarize,

5
00:00:17.138 --> 00:00:20.954
by showing it examples of
human generated summaries.

6
00:00:20.954 --> 00:00:27.194
In 2020, researchers at OpenAI published a
paper that explored the use of fine-tuning

7
00:00:27.194 --> 00:00:32.706
with human feedback to train a model to
write short summaries of text articles.

8
00:00:32.706 --> 00:00:36.887
Here you can see that a model
fine-tuned on human feedback produced

9
00:00:36.887 --> 00:00:39.601
better responses than a pretrained model,

10
00:00:39.601 --> 00:00:45.150
an instruct fine-tuned model, and
even the reference human baseline.

11
00:00:45.150 --> 00:00:50.697
A popular technique to finetune large
language models with human feedback is

12
00:00:50.697 --> 00:00:56.000
called reinforcement learning from
human feedback, or RLHF for short.

13
00:00:57.010 --> 00:01:01.815
As the name suggests, RLHF uses
reinforcement learning, or RL for

14
00:01:01.815 --> 00:01:05.693
short, to finetune the LLM
with human feedback data,

15
00:01:05.693 --> 00:01:11.510
resulting in a model that is better
aligned with human preferences.

16
00:01:11.510 --> 00:01:16.243
You can use RLHF to make sure that
your model produces outputs that

17
00:01:16.243 --> 00:01:21.130
maximize usefulness and
relevance to the input prompt.

18
00:01:21.130 --> 00:01:27.550
Perhaps most importantly, RLHF can
help minimize the potential for harm.

19
00:01:27.550 --> 00:01:32.726
You can train your model to give caveats
that acknowledge their limitations and

20
00:01:32.726 --> 00:01:35.130
to avoid toxic language and topics.

21
00:01:36.190 --> 00:01:41.858
One potentially exciting application of
RLHF is the personalizations of LLMs,

22
00:01:41.858 --> 00:01:46.610
where models learn the preferences
of each individual user through

23
00:01:46.610 --> 00:01:49.330
a continuous feedback process.

24
00:01:49.330 --> 00:01:54.182
This could lead to exciting new
technologies like individualized learning

25
00:01:54.182 --> 00:01:57.270
plans or personalized AI assistants.

26
00:01:57.270 --> 00:02:01.845
But in order to understand how these
future applications might be made

27
00:02:01.845 --> 00:02:06.890
possible, let's start by taking
a closer look at how RLHF works.

28
00:02:06.890 --> 00:02:10.288
In case you aren't familiar
with reinforcement learning,

29
00:02:10.288 --> 00:02:14.330
here's a high level overview of
the most important concepts.

30
00:02:14.330 --> 00:02:20.258
Reinforcement learning is a type of
machine learning in which an agent learns

31
00:02:20.258 --> 00:02:26.551
to make decisions related to a specific
goal by taking actions in an environment,

32
00:02:26.551 --> 00:02:31.950
with the objective of maximizing
some notion of a cumulative reward.

33
00:02:33.010 --> 00:02:38.134
In this framework, the agent continually
learns from its experiences by

34
00:02:38.134 --> 00:02:43.425
taking actions, observing the resulting
changes in the environment, and

35
00:02:43.425 --> 00:02:48.482
receiving rewards or penalties,
based on the outcomes of its actions.

36
00:02:48.482 --> 00:02:54.194
By iterating through this process, the
agent gradually refines its strategy or

37
00:02:54.194 --> 00:02:59.160
policy to make better decisions and
increase its chances of success.

38
00:03:00.410 --> 00:03:05.874
A useful example to illustrate these ideas
is training a model to play Tic-Tac-Toe.

39
00:03:05.874 --> 00:03:07.036
Let's take a look.

40
00:03:07.036 --> 00:03:13.710
In this example, the agent is a model or
policy acting as a Tic-Tac-Toe player.

41
00:03:13.710 --> 00:03:17.230
Its objective is to win the game.

42
00:03:17.230 --> 00:03:20.706
The environment is the three
by three game board, and

43
00:03:20.706 --> 00:03:25.890
the state at any moment,
is the current configuration of the board.

44
00:03:25.890 --> 00:03:30.284
The action space comprises all
the possible positions a player can

45
00:03:30.284 --> 00:03:33.510
choose based on the current board state.

46
00:03:33.510 --> 00:03:39.190
The agent makes decisions by following
a strategy known as the RL policy.

47
00:03:39.190 --> 00:03:44.093
Now, as the agent takes actions,
it collects rewards based on the actions'

48
00:03:44.093 --> 00:03:47.148
effectiveness in
progressing towards a win.

49
00:03:47.148 --> 00:03:52.254
The goal of reinforcement learning is for
the agent to learn the optimal policy for

50
00:03:52.254 --> 00:03:55.850
a given environment that
maximizes their rewards.

51
00:03:55.850 --> 00:04:00.430
This learning process is iterative and
involves trial and error.

52
00:04:00.430 --> 00:04:05.550
Initially, the agent takes a random
action which leads to a new state.

53
00:04:05.550 --> 00:04:06.626
From this state,

54
00:04:06.626 --> 00:04:11.890
the agent proceeds to explore subsequent
states through further actions.

55
00:04:11.890 --> 00:04:16.594
The series of actions and
corresponding states form a playout,

56
00:04:16.594 --> 00:04:18.850
often called a rollout.

57
00:04:18.850 --> 00:04:23.598
As the agent accumulates experience,
it gradually uncovers actions that yield

58
00:04:23.598 --> 00:04:28.328
the highest long-term rewards,
ultimately leading to success in the game.

59
00:04:28.328 --> 00:04:32.990
Now let's take a look at how
the Tic-Tac-Toe example can be extended

60
00:04:32.990 --> 00:04:37.298
to the case of fine-tuning large
language models with RLHF.

61
00:04:37.298 --> 00:04:42.372
In this case, the agent's policy
that guides the actions is the LLM,

62
00:04:42.372 --> 00:04:46.930
and its objective is to generate
text that is perceived as being

63
00:04:46.930 --> 00:04:50.350
aligned with the human preferences.

64
00:04:50.350 --> 00:04:56.990
This could mean that the text is, for
example, helpful, accurate, and non-toxic.

65
00:04:56.990 --> 00:05:00.707
The environment is the context
window of the model,

66
00:05:00.707 --> 00:05:04.690
the space in which text can
be entered via a prompt.

67
00:05:04.690 --> 00:05:10.074
The state that the model considers before
taking an action is the current context.

68
00:05:10.074 --> 00:05:15.270
That means any text currently
contained in the context window.

69
00:05:15.270 --> 00:05:19.190
The action here is the act
of generating text.

70
00:05:19.190 --> 00:05:23.360
This could be a single word,
a sentence, or a longer form text,

71
00:05:23.360 --> 00:05:26.810
depending on the task
specified by the user.

72
00:05:26.810 --> 00:05:31.536
The action space is the token vocabulary,
meaning all the possible tokens

73
00:05:31.536 --> 00:05:35.474
that the model can choose from
to generate the completion.

74
00:05:35.474 --> 00:05:39.612
How an LLM decides to generate the next
token in a sequence, depends on

75
00:05:39.612 --> 00:05:45.150
the statistical representation of language
that it learned during its training.

76
00:05:45.150 --> 00:05:49.954
At any given moment, the action that
the model will take, meaning which

77
00:05:49.954 --> 00:05:54.679
token it will choose next, depends on
the prompt text in the context and

78
00:05:54.679 --> 00:05:59.330
the probability distribution
over the vocabulary space.

79
00:05:59.330 --> 00:06:03.913
The reward is assigned based on how
closely the completions align with human

80
00:06:03.913 --> 00:06:05.510
preferences.

81
00:06:05.510 --> 00:06:08.577
Given the variation in human
responses to language,

82
00:06:08.577 --> 00:06:13.990
determining the reward is more complicated
than in the Tic-Tac-Toe example.

83
00:06:13.990 --> 00:06:17.230
One way you can do this is
to have a human evaluate

84
00:06:17.230 --> 00:06:21.847
all of the completions of the model
against some alignment metric,

85
00:06:21.847 --> 00:06:27.210
such as determining whether the generated
text is toxic or non-toxic.

86
00:06:27.210 --> 00:06:33.310
This feedback can be represented as
a scalar value, either a zero or a one.

87
00:06:33.310 --> 00:06:37.831
The LLM weights are then updated
iteratively to maximize the reward

88
00:06:37.831 --> 00:06:40.370
obtained from the human classifier,

89
00:06:40.370 --> 00:06:45.010
enabling the model to generate
non-toxic completions.

90
00:06:45.010 --> 00:06:50.370
However, obtaining human feedback
can be time consuming and expensive.

91
00:06:50.370 --> 00:06:55.454
As a practical and scalable alternative,
you can use an additional model,

92
00:06:55.454 --> 00:06:59.743
known as the reward model,
to classify the outputs of the LLM and

93
00:06:59.743 --> 00:07:04.550
evaluate the degree of alignment
with human preferences.

94
00:07:04.550 --> 00:07:08.622
You'll start with a smaller number of
human examples to train the secondary

95
00:07:08.622 --> 00:07:12.170
model by your traditional
supervised learning methods.

96
00:07:12.170 --> 00:07:17.419
Once trained, you'll use the reward model
to assess the output of the LLM and

97
00:07:17.419 --> 00:07:22.990
assign a reward value, which in turn gets
used to update the weights off the LLM and

98
00:07:22.990 --> 00:07:26.350
train a new human aligned version.

99
00:07:26.350 --> 00:07:30.748
Exactly how the weights get updated as
the model completions are assessed,

100
00:07:30.748 --> 00:07:34.290
depends on the algorithm
used to optimize the policy.

101
00:07:34.290 --> 00:07:37.810
You'll explore these issues
in more depth shortly.

102
00:07:37.810 --> 00:07:41.155
Lastly, note that in the context
of language modeling,

103
00:07:41.155 --> 00:07:44.429
the sequence of actions and
states is called a rollout,

104
00:07:44.429 --> 00:07:49.410
instead of the term playout that's used
in classic reinforcement learning.

105
00:07:49.410 --> 00:07:54.212
The reward model is the central component
of the reinforcement learning process.

106
00:07:54.212 --> 00:07:59.297
It encodes all of the preferences that
have been learned from human feedback, and

107
00:07:59.297 --> 00:08:04.730
it plays a central role in how the model
updates its weights over many iterations.

108
00:08:04.730 --> 00:08:08.840
In the next video, you'll see how this
model is trained and how you use it to

109
00:08:08.840 --> 00:08:13.292
classify the model's outputs during
the reinforcement learning process.

110
00:08:13.292 --> 00:08:14.580
Let's move on and take a look.