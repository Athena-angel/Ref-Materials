WEBVTT

1
00:00:00.000 --> 00:00:02.520
Everything you've seen
so far in the course,

2
00:00:02.520 --> 00:00:05.025
from selecting your
model to fine tuning it,

3
00:00:05.025 --> 00:00:07.170
and aligning it with
human preferences,

4
00:00:07.170 --> 00:00:10.080
will happen before you
deploy your application.

5
00:00:10.080 --> 00:00:12.510
To help you plan
out these stages

6
00:00:12.510 --> 00:00:14.865
of the generative AI
project life cycle,

7
00:00:14.865 --> 00:00:17.280
this cheat sheet provide
some indication of

8
00:00:17.280 --> 00:00:21.420
the time and effort required
for each phase of work.

9
00:00:21.420 --> 00:00:23.430
As you saw earlier,

10
00:00:23.430 --> 00:00:26.955
pre-training a large language
model can be a huge effort.

11
00:00:26.955 --> 00:00:29.100
This stage is the
most complex you'll

12
00:00:29.100 --> 00:00:31.710
face because of the model
architecture decisions,

13
00:00:31.710 --> 00:00:34.120
the large amount of
training data required,

14
00:00:34.120 --> 00:00:36.390
and the expertise needed.

15
00:00:36.390 --> 00:00:38.720
Remember though,
that in general,

16
00:00:38.720 --> 00:00:40.280
you will start your
development work with

17
00:00:40.280 --> 00:00:42.200
an existing foundation model.

18
00:00:42.200 --> 00:00:45.220
You'll probably be able
to skip this stage.

19
00:00:45.220 --> 00:00:47.855
If you're working with
a foundation model,

20
00:00:47.855 --> 00:00:49.250
you'll likely start to assess

21
00:00:49.250 --> 00:00:51.995
the model's performance
through prompt engineering,

22
00:00:51.995 --> 00:00:54.665
which requires less
technical expertise,

23
00:00:54.665 --> 00:00:57.205
and no additional
training of the model.

24
00:00:57.205 --> 00:01:00.080
If your model isn't
performing as you need,

25
00:01:00.080 --> 00:01:03.790
you'll next think about prompt
tuning and fine tuning.

26
00:01:03.790 --> 00:01:06.185
Depending on your use case,

27
00:01:06.185 --> 00:01:08.555
performance goals,
and compute budget,

28
00:01:08.555 --> 00:01:10.280
the methods you'll
try could range

29
00:01:10.280 --> 00:01:12.050
from full fine-tuning to

30
00:01:12.050 --> 00:01:14.330
parameter efficient
fine tuning techniques

31
00:01:14.330 --> 00:01:16.540
like laura or prompt tuning.

32
00:01:16.540 --> 00:01:19.070
Some level of
technical expertise

33
00:01:19.070 --> 00:01:20.580
is required for this work.

34
00:01:20.580 --> 00:01:22.700
But since fine-tuning
can be very

35
00:01:22.700 --> 00:01:25.880
successful with a relatively
small training dataset,

36
00:01:25.880 --> 00:01:27.440
this phase could potentially be

37
00:01:27.440 --> 00:01:29.375
completed in a single day.

38
00:01:29.375 --> 00:01:32.090
Aligning your model
using reinforcement

39
00:01:32.090 --> 00:01:35.000
learning from human feedback
can be done quickly,

40
00:01:35.000 --> 00:01:37.445
once you have your
train reward model.

41
00:01:37.445 --> 00:01:39.320
You'll likely see if you can use

42
00:01:39.320 --> 00:01:41.540
an existing reward
model for this work,

43
00:01:41.540 --> 00:01:43.535
as you saw in this week's lab.

44
00:01:43.535 --> 00:01:45.080
However, if you have to

45
00:01:45.080 --> 00:01:47.195
train a reward
model from scratch,

46
00:01:47.195 --> 00:01:49.070
it could take a long
time because of

47
00:01:49.070 --> 00:01:51.970
the effort involved to
gather human feedback.

48
00:01:51.970 --> 00:01:54.440
Finally, optimization techniques

49
00:01:54.440 --> 00:01:56.330
you learned about
in the last video,

50
00:01:56.330 --> 00:01:57.770
typically fall in the middle in

51
00:01:57.770 --> 00:01:59.750
terms of complexity and effort,

52
00:01:59.750 --> 00:02:02.240
but can proceed quite
quickly assuming

53
00:02:02.240 --> 00:02:03.710
the changes to the model don't

54
00:02:03.710 --> 00:02:05.830
impact performance too much.

55
00:02:05.830 --> 00:02:08.135
After working through
all of these steps,

56
00:02:08.135 --> 00:02:10.055
you have hopefully
trained in tuned

57
00:02:10.055 --> 00:02:11.930
a gray LLM that is working

58
00:02:11.930 --> 00:02:13.970
well for your specific use case,

59
00:02:13.970 --> 00:02:16.460
and is optimized for deployment.

60
00:02:16.460 --> 00:02:18.205
Congratulations.

61
00:02:18.205 --> 00:02:21.410
In the final sequence of
videos in this course,

62
00:02:21.410 --> 00:02:23.750
you'll explore the
remaining problems with

63
00:02:23.750 --> 00:02:26.470
LLM performance that you
may need to address,

64
00:02:26.470 --> 00:02:28.600
before launching
your application.

65
00:02:28.600 --> 00:02:31.480
As well as techniques
that can overcome them.

66
00:02:31.480 --> 00:02:34.210
Let's move on and take a look.