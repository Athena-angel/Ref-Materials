In the previous section, you saw how LLMs can interact
with external datasets. Now let's take a
look at how they can interact with external
applications. To motivate the types of
problems and use cases that require this kind of
augmentation of the LLM, you'll revisit the customer
service bot example you saw earlier in the course. During this walkthrough of one customer's
interaction with ShopBot, you'll take a look at the integrations that
you'd need to allow the app to process a return
requests from end to end. In this conversation,
the customer has expressed that they
want to return some genes that they purchased. ShopBot responds by asking
for the order number, which the customer
then provides. ShopBot then looks up the order number in the
transaction database. One way it could do
this is by using a rag implementation
of the kind you saw earlier in the
previous video. In this case here, you would likely be retrieving data through a SQL query to a back-end order
database rather than retrieving data from a
corpus of documents. Once ShopBot has retrieved
the customers order, the next step is to confirm the items that
will be returned. The bot ask the
customer if they'd like to return anything
other than the genes. After the user
states their answer, the bot initiates a request to the company's shipping
partner for a return label. The body uses the shippers
Python API to request the label ShopBot is going to email the shipping
label to the customer. It also asks them to confirm
their email address. The customer responds with their email address and the bot includes this information in
the API call to the shipper. Once the API request
is completed, the Bartlett's the
customer know that the label has been
sent by email, and the conversation
comes to an end. This short example illustrates
just one possible set of interactions that you might need an LLM to be capable of
to power and application. In general, connecting LLMs to external applications
allows the model to interact with
the broader world, extending their utility
beyond language tasks. As the shop bought
example showed, LLMs can be used
to trigger actions when given the ability
to interact with APIs. LLMs can also connect to
other programming resources. For example, a
Python interpreter that can enable models to incorporate accurate
calculations into their outputs. It's important to
note that prompts and completions are at the very
heart of these workflows. The actions that the app
will take in response to user requests will be
determined by the LLM, which serves as the
application's reasoning engine. In order to trigger actions, the completions generated by the LLM must contain certain
important information. First, the model needs to be
able to generate a set of instructions so that
the application knows what actions to take. These instructions need to be understandable and correspond
to allowed actions. In the ShopBot
example for instance, the important steps were; checking the order ID, requesting a shipping label, verifying the user email, and emailing the user the label. Second, the completion
needs to be formatted in a way that the broader
application can understand. This could be as simple as a specific sentence
structure or as complex as writing a script in Python or generating
a SQL command. For example, here is a
SQL query that would determine whether an order is present in the database
of all orders. Lastly, the model
may need to collect information that allows
it to validate an action. For example, in the
ShopBot conversation, the application needed
to verify the email address the customer used
to make the original order. Any information that
is required for validation needs to
be obtained from the user and contained in the completion so it can be passed through to
the application. Structuring the prompts
in the correct way is important for all of
these tasks and can make a huge difference in the
quality of a plan generated or the adherence to a desired
output format specification